% Based on TIT-LaTeX-template-202312

% Comment out one of the following two lines for 1/2 columns
%\documentclass[lettersize,onecolumn]{IEEEtran}
\documentclass[lettersize,journal]{IEEEtran}

\usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsthm}   % Math
\usepackage{geometry}           % Page margins
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% BibLaTeX for references (requires biber)
\usepackage[
    backend=biber,
    style=numeric,
    sorting=nyt
]{biblatex}

\addbibresource{references.bib}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\unif}[1]{\mathrm{Unif}\{#1\}}
\newcommand{\bern}[1]{\mathrm{Bern}\{#1\}}
\newcommand{\entropy}[1]{\mathrm{H}(#1)}
\newcommand{\prob}[1]{\mathbb{P}(#1)}
\newcommand{\expected}[1]{\mathbb{E}(#1)}

\lstset{
  language=C,        % choose the language
  basicstyle=\ttfamily\small, % font style and size
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{orange},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  breaklines=true,
  frame=single,
  captionpos=b
}

\begin{document}

% Title
\title{Efficient random number generation using an entropy store}
\author{Calum Grant}
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% 
%\thanks{Manuscript submitted Mmmmmm XX, 2025.}
% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~1, No.~2, December~2023}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000~\copyright~2023 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.


\maketitle

\begin{abstract}
    We present new algorithms for generating random integers, that are extremely efficient from an entropy perspective.
    This means that less entropy needs to be read from an external source for each random number generated.
    The method introduces an \em entropy store \em that is available to cache unused entropy between invocations, and allows algorithms to use a more efficient strategy.
    The method can generate perfect random variates for any weighted integer distribution, and is asymptotically optimal. 
    For example we can shuffle a deck of 52 cards using just $\approxeq 225.58102$ bits of entropy, yielding an entropy  efficiency of $\approxeq 0.99999992$ using a 32-bit entropy store, compared with classical algorithms that only have an efficiency of $\approxeq 0.81$. An entropy store allows us to bypass the classical limits on entropy efficiency established by Knuth and Yao, and has practical applications when generating variates directly from hardware-based entropy which is often a performance bottleneck.    
\end{abstract}

\begin{IEEEkeywords}
    Bernoulli distribution, discrete random variable, entropy conversion, entropy efficiency, entropy store, random number generation, uniform distribution, weighted distribution.
\end{IEEEkeywords}

\section{Introduction}

In this paper, we will study generating perfectly distributed integers from an entropy source. A typical example of this is using fair coin flips to roll a fair die or to perform a perfect shuffle of a deck of cards, but random numbers are ubiquitous in other areas such as security, AI, and simulation.

The general problem is one of \em entropy conversion\em, where entropy in one form $X$ needs to be converted to entropy in a different form $f(X)$, where $X$ is a discrete random $X$ having Shannon entropy $\entropy{X}$
\cite{shannon1948mathematical}.  
A fundamental property is that you cannot get more entropy out than you put in, i.e. $\entropy{X} \ge \entropy{f(X)}$. \cite{cover1999elements} 
We will be mainly focussed on improving the \em efficiency \em of entropy conversion, defined as $\eta = \frac{\entropy{f(X)}}{\entropy{X}}$.

Whilst this problem has been studied extensively, there are theoretical limits on efficiency because algorithms must always fetch entropy in quantised units, and any excess entropy is unused. An optimal algorithm for generating uniform variables from coin flips must fetch up to 2 extra bits of entropy per output.  \cite{cover1999elements, Knuth1976TheCO}

To mitigate these entropy losses, it is possible to generate random variables in batches. The major drawback with batching schemes is that they do not scale very well and have limits on their capacity and efficiency.

In this paper we will explore a fundamentally different approach, by allowing entropy conversion algorithms access to an \em entropy store \em (ES) in the form of a large uniformly distributed integer variable, and allow the algorithm to put back any unused entropy into the store. By first looking at the available operations on a uniform variable, we can compose them in such a way as to minimise the entropy loss.

The entropy efficiency depends only on the ratio of the size of the store to the size of the number generated, and a precise bound is given in Theorem \ref{thm:loss}. 

For example to roll a 6-sided die with a 32-bit entropy store (holding at least 31 bits of entropy) has an entropy efficiency of $>0.99999997$. To shuffle a deck of 52 cards with a 32-bit entropy store has an entropy efficiency $>0.99999992$. By increasing the size of the store, we can achieve entropy efficiency arbitrarily close to 1.


\subsection {Contribution}

We present new algorithms, called \em entropy store algorithms\em, that have lower setup costs and higher amortised entropy efficiency than classical algorithms. Table \ref{tab:entropy-store} summarises the algorithm, where $m$ is the number of bits in each integer, $n$ is the size of the weighted distribution $W$. $\epsilon$ is defined in Theorem \ref{thm:loss} and graphed in Figure \ref{fig:uniform-losses}. These characteristics are essentially optimal since $m$ and $n$ are usually constant.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Distribution & Uniform & Bernoulli & Weighted \\
\hline
Input & \multicolumn{3}{|c|}{Unbiassed coin or dice} \\
Output & \multicolumn{3}{|c|}{Exact} \\
Entropy in & \multicolumn{3}{|c|}{$\entropy{W}+\epsilon$ (amortised)} \\
\hline
Setup time & \multicolumn{2}{|c|}{$O(m)$} & $O(nm)$ \\
Space & \multicolumn{2}{|c|}{$O(m)$} & $O(nm)$ \\
Per output & \multicolumn{2}{|c|}{$O(m \log m)$} & $O(m \log m)$ \\
\hline
\end{tabular}
\caption{Entropy store algorithm characteristics.}
    \label{tab:entropy-store}
\end{table}

We show that this algorithm is faster other random number generators when reading directly from a hardware entropy source, where the limiting factor is the rate of entropy input.

\subsection{Related work}

Entropy conversion is a well studied subject starting when Von Neumann \cite{neumann51} proposed the first known algorithm for generating perfectly uniform variables (a "dice roll") from an unbiassed coin (a "coin flip") - shortly after Claude Shannon's seminal work on entropy \cite{shannon1948mathematical}. Von Neumann's rejection sampling algorithm works by fetching the smallest power of 2 greater or equal to the number being generated. If the number is in range, return it, otherwise retry. This algorithm is elegant but not optimal in its entropy efficiency.

Knuth and Yao \cite{Knuth1976TheCO} devised an optimal algorithm, using a binary decision tree based on an inverse Huffman encoding \cite{huffman52} of each output, and proved that no other binary decision tree could on average use fewer decisions.

The drawbacks with the original Knuth-Yao algorithm are the setup and space costs: the decision trees are explicitly constructed can get quite large. \cite{saad2025} Lumbruso \cite{lumbroso2013optimal} devised an optimal dice rolling algorithm, that is no more efficient than Knuth-Yao, but requires no setup and is simple to implement. Bacher \emph{et al.} \cite{bacher2017} analyse the Von Neumann and Knuth-Yao algorithms from an entropy perspective, using Lumbruso's implementation, with a particular focus on generating unbiased permutations from unbiased coin flips. There are always up to 2 bits of entropy loss (or "toll") for each uniform variable generated, depending on the number being generated. Only exact powers of 2 can be generated without entropy loss.

Lemire \cite{lemire2019fast} compares different practical uniform generators, and describes a uniform generator that is efficient from a CPU perspective, but is very inefficient from an entropy perspective, by avoiding CPU division operations. Lemire's use case is for entropy that comes from a pseudorandom source that can be quickly generated.

Various methods have been explored for generating arbitrary weighted distributions. The Knuth-Yao algorithm can be used to generate arbitrary distributions from binary entropy.

The \emph{alias method} devised by Walker \cite{walker1977efficient} and optimized by Vose \cite{vose91} maps a uniform variable to an output, and then uses a second Bernoulli variable to look up an aliased output. This is $O(n)$ in setup time, and $O(1)$ in output time, but is not optimal in terms of entropy efficiency.

Han and Hoshi \cite{han97} devised an optimal interval algorithm to convert biased or unbiased coin-flips to an arbitrary distribution, with an overhead of up to 3 bits of entropy per output.  The algorithm works by dividing the $[0,1)$ real interval according to the output distribution, and stops at bit $n$ when the fetched binary number falls entirely within the output interval in the $n^\text{th}$ bit. Han and Hoshi show that this algorithm has an optimal entropy consumption. 
Wanatabe \cite{wanatabe20} analyses this algorithm using an information spectrum, and Oohama \cite{oohama11, oohama2020performance} analyses the performance this algorithm. Han and Hoshi's algorithm can also be used to read biassed inputs and form part of a Markov chain.

Saad \emph{et al.} \cite{saad2020fldr} introduce Fast Loaded Dice Roller (FLDR) algorithm and Draper and Saad \cite{draper2025efficient} the Amplified Loaded Dice Roller (ALDR) algorithm to generate more efficient discrete distribution generating trees (DDG). They observe that DDG-based algorithms like Knuth-Yao can be very large and slow, so propose a table-based approach with optimizations, which creates a faster random distribution generator within 2 bits per sample of the output entropy for ALDR, and 6 bits per output for FLDR.

\em Entropy extraction \em studies how to convert the entropy \em from \em a variety of different distributions. When the input distribution is unknown, Von Neumann \cite{neumann51} gives an algorithm to extract unbiased entropy from a biassed coin, which is again elegant but suboptimal in its entropy efficiency. Peres \cite{peres1992iterating} devised a way to recursively use the previously-discarded outputs to provide an algorithm with perfect amortised entropy efficiency, and the method was generalised by Pae \cite{pae15} to extract entropy from arbitrary but identically distributed unknown distributions ("weighted M-dice") with perfect amortised efficiency. Interestingly, it appears to be easier to extract entropy than to generate it.

Universal hashing can be used to turn unknown distributions into fair bits. Vembu \emph{et} al. \cite{vembu95} analyse the maximum rate at which entropy can be extracted. Goldreich's Leftover hash lemma \cite{goldreich2004foundations} calculates the length of possible output. Trevisan \cite{trevisan2001extractors} implements a universal hash extractor for unknown distributions with a known entropy. 

A common proposal to reach asymptotic efficiency for random number generation is to use batching to generate multiple outputs at the same time. \cite{bacher2017,devroye86,han97,Knuth1976TheCO,lumbroso2013optimal} Batching spreads the entropy loss over the batch size. However batching schemes suffer from increased size and complexity, and are less flexible because you must plan beforehand which numbers to generate. The fundamental limit to batching is that they do not scale well because they require arbitrary floating point precision or data structure sizes.

Fill and Huber \cite{fill2000randomness, huber2016perfect}, describe the \em randomness recycler \em protocol which is a general purpose strategy to identify excess entropy in an algorithm and store it for the next state. This idea has been used to create entropy-optimal uniform \cite{lumbroso2013optimal, huber2024optimalrollingfairdice} or weighted \cite{huber2024optimalrollingfairdice} variables, but entropy is not recycled between invocations, so their method does not improve on the efficiency established by Knuth-Yao and Han-Hoshi.


\section{Algorithms for entropy conversion}

In this section we'll define some basic building blocks (Algorithm \ref{alg:combine}-\ref{alg:generate-multiple}), and then use these to create an efficient generator for uniform variables (Algorithm \ref{alg:generate-uniform}).

We'll then build on Algorithm \ref{alg:generate-multiple} to create a generator for Bernoulli variables (Algorithm \ref{alg:generate-bernoulli}) and weighted variables (Algorithm \ref{alg:generate-weighted}). All of these algorithms are designed to have a 1 or near-1 entropy efficiency, and proofs of correctness and efficiency are contained throughout this section.

We'll use uppercase letters $X$, $Y$ and $Z$ for discrete random variables, and write $\entropy{X}$ for the Shannon entropy of this variable. Write $X \sim \unif{0 ... n-1}$ to mean that $X$ has a discrete uniform distribution of size $n$, or $X \sim \unif{n}$ for short. $\entropy{\unif{n}} = \log_2n$. $\bern{p}$ is a Bernoulli distribution with parameter $p$. $A \indep B$ means that random variables $A$ and $B$ are independent.  The interval $[a,b)$ means all integers in the range $a...b-1$. Write $\mathbb{P}$ for probability, $\mathbb{R}$ for the set of real numbers, and lowercase $f$ as a function on random variables. In algorithms, $\div$ and $\mod$ are integer division and modulus operations, and subscripts $_{[...]}$ are used for array access.

We'll say "uniform variable" to mean a uniformly distributed integer variable or variate.


\subsection{Basic operations}

At the basis of the ES is the ability to transform a uniform variable into different forms.  We can divide a uniform variable into (a) two uniform variables, (b) a Bernoulli variable and a uniform variable, (c) a weighted variable and a uniform variable. These transformations preserve entropy. The contribution is how to combine these transformations for efficient entropy conversion.

The function $f_{combine}$, in Definition \ref{def:combine}, allows us two combine two uniform variables $X$ and $Y$ into a single uniform variable $Z$. Its inverse $f_{divide}$ is given in Definition \ref{def:divide}. $f_{combine}$ and $f_{divide}$ divide an interval of size $nm$ into $n$ intervals of size $m$ as follows:

\[
\overbrace{    
    \underbrace{
        \underbrace{z_0 ... z_{m-1}}_{Y \sim \unif{m}}
        \underbrace{z_m ... z_{2m-1}}_{Y \sim \unif{m}}
        ...
        \underbrace{z_{n(m-1)}...z_{nm-1}}_{Y \sim \unif{m}}    
    }}_{X \sim \unif{n}}
^{Z \sim \unif{nm}}
\]

\begin{definition}
    Let $f_{combine}: [0,n)\times [0,m) \rightarrow [0,nm)$ be $f_{combine}: (X,Y) \mapsto mX+Y$.
    \label{def:combine}
\end{definition}

\begin{definition}
    Let $f_{divide}: [0,nm) \rightarrow [0,n) \times [0,m)$ be $f_{divide}: Z \mapsto (\lfloor \frac{Z}{m} \rfloor, Z \mod m)$.
    \label{def:divide}
\end{definition}

\begin{lemma}
    $f_{combine}$ is a bijection with inverse $f_{divide}.$
\end{lemma}

\begin{proof}
    $f_{combine}(f_{divide}(Z)) = f_{combine}(\lfloor \frac{Z}{m}\rfloor, Z \text{ mod } m) = m\lfloor \frac{Z}{m} \rfloor + (Z \mod m) = Z$. $f_{divide}(f_{combine}(X,Y)) = f_{divide}(mX+Y) = (\lfloor\frac{mX+Y}{m}\rfloor, (mX+Y)\mod m) = (\lfloor X+\frac{Y}{m}\rfloor, Y) = (X,Y)$. Therefore $f_{combine}$ is a bijection.
\end{proof}

\begin{lemma}
    If $X \sim \unif{n}$, $Y \sim \unif{m}$ and $X \indep Y$, then 
    $f_{combine}(X,Y) \sim \unif{nm}$.
    \label{lem:combine}
\end{lemma}

\begin{proof}
    Let $Z = mX+Y$, so $Z \in [0,nm)$. $\prob{X=x,Y=y} = \prob{X=x}\mathbb{P}(Y=y) = \frac{1}{n}\frac{1}{m} = \frac{1}{nm}$. Since $f_{combine}$ is a bijection then it means that all $z$ values occur with $\prob{Z=z} = \frac{1}{nm}$, so $Z \sim \unif{nm}$.
    
\end{proof}

\begin{lemma}
    If $Z \sim \unif{nm}$, and $f_{divide}(Z) = (X,Y)$, then $X \sim \unif{n}$ and $Y \sim \unif{m}$. $X \indep Y$.
    \label{lem:divide-postconditions}
\end{lemma}

\begin{proof}
    $\prob{X=x,Y=y} = \frac{1}{nm}$. $\prob{X=x} = \sum_{y}\prob{X=x,Y=y} = m\frac{1}{nm} = \frac{1}{n}$. $\prob{Y=y} = \sum_{x}\prob{X=x,Y=y} = n\frac{1}{nm} = \frac{1}{m}$. Therefore $X\sim \unif{n}$ and $Y\sim \unif{m}$.

    $\mathbb{P}(X=x,Y=y) = \frac{1}{nm} = \mathbb{P}(X=x)\mathbb{P}(Y=y)$, so $X \indep Y$.
\end{proof}

\begin{corollary}
    $f_{combine}$ and $f_{divide}$ do not lose entropy.
    \label{cor:combine-entropy}
\end{corollary}

\begin{proof}$f_{combine}$ and $f_{divide}$ are bijections so must preserve entropy. $\entropy{X,Y} = \entropy{X} + \entropy{Y} = \log_2{n} + \log_2{m} = \log_2(nm) = \entropy{Z}$, so the entropy on both sides of the transformation is equal.
\end{proof}

Algorithm \ref{alg:combine} is an implementation of $f_{combine}$, and Algorithm \ref{alg:divide} is an implementation of $f_{divide}$, with the postconditions established by Lemmas \ref{lem:combine} and \ref{lem:divide-postconditions}.

\begin{algorithm}
\caption{Combining two uniform variables into one uniform variable}
\label{alg:combine}
\begin{algorithmic}[1]
    \Require $X, Y, n$ and $m$ are integers
    \Require $X \sim \unif{n}$
    \Require $Y \sim \unif{m}$
    \Require $X \indep Y$
    \Ensure $z$ is $x * y$
    \Ensure $Z \sim \unif{z}$
\Procedure{combine}{$X, n, Y, m$} 
  \State $Z \gets m * X + Y$
  \State $z \gets n * m$
  \State \Return $Z, z$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Converting a uniform variable into two uniform variables by division}
\label{alg:divide}
\begin{algorithmic}[1]
    \Require $Z, z$ and $n$ are integers
    \Require $z \mod m = 0$, i.e. $z$ is divisible by $m$
    \Require $Z \sim \unif{z}$
    \Ensure $n * m = z$
    \Ensure $X \sim \unif{n}$
    \Ensure $Y \sim \unif{m}$
    \Ensure $X \indep Y$
\Procedure{divide}{$Z, z, m$} 
  \State $X \gets Z \div m$
  \State $Y \gets Z \mod m$
  \State $n \gets z \div m$
  \State \Return $X, n, Y$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The function $f_{sample}$, in Definition \ref{def:sample}, divides the interval $[0,n)$ into the intervals $[0,m)$ and $[m, n)$ as follows:

\[
\overbrace{
    \underbrace{
        \underbrace{z_0 \text{   } ... \text{   } z_{m-1}}_{Y_0 \sim \unif{m}}
        \underbrace{z_m \text{   } ... \text{   } z_{n-1}}_{Y_1 \sim \unif{n-m}}}
    }_{X \sim \bern{\frac{m}{n}}}^{Z \sim \unif{n}}
\]

\begin{definition}
Let $f_{sample}: Z \mapsto (Z<m, Z - m(Z\ge m))$. The operators $<$ and $\ge$ evaluate to $0$ or $1$.
\label{def:sample}
\end{definition}

\begin{lemma}
If $Z \sim \unif{n}$ and $f_{sample}(Z) = (X,Y)$, then $X \sim \bern{\frac{m}{n}}$ and $Y \sim \unif{m(Z \ge m)+Z}$. $X \indep Y$.
\label{lem:sample}
\end{lemma}

\begin{proof}
    $\prob{X=0} = \prob{Z < m} = \frac{m}{n}$, $\prob{X=1} = \prob{Z \ge m} = \frac{n-m}{n} = 1 - \frac{m}{n} = 1 - \prob{X=0}$, therefore $X \sim \bern{\frac{m}{n}}$.

    If $X=0$, then $Z<m$ and $\prob{Y=y} = \prob{Z=y | Z<m} = \frac{1}{m}$, therefore $Y \sim \unif{m}$. If $X=1$ then $Z\ge m$ and $\prob{Y=y} = \prob{Z = Y + m| Z\ge m} = \frac{1}{n-m}$, therefore $Y \sim \unif{n-m}$. In both cases $X \indep Y$.
\end{proof}

Algorithm \ref{alg:sample} implements $f_{sample}$. Algorithm \ref{alg:sample} can be used to resize a uniform variable or to generate a Bernoulli variable.

\begin{algorithm}
\caption{Converting a uniform variable into a Bernoulli and a uniform variable}
\label{alg:sample}
\begin{algorithmic}[1]
    \Require $Z, n$ and $m$ are integers 
    \Require $0 \le m \le n$
    \Require $Z \sim \unif{n}$
    \Ensure $X \sim \bern{\frac{m}{n}}$
    \Ensure $Y \sim \unif{y}$
    \Ensure $y = Xm + (1-X)(n-m)$
    \Ensure $X \indep Y$
\Procedure{sample}{$Z, n, m$} 
  \If{$Z < m$}
    \State $X \gets 1$  
    \State $y \gets m$
    \State $Y \gets Z$
  \Else
    \State $X \gets 0$  
    \State $y \gets n-m$
    \State $Y \gets Z-m$
  \EndIf
  \State \Return $Y, y, X$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
    $f_{sample}$ preserves entropy.
    \label{lem:sample-entropy}
\end{lemma}

\begin{proof}If $f_{sample}(Z) = (X,Y)$, then the entropy in $X \sim \bern{\frac{m}{n}}$ is given by the binary entropy function \cite{cover1999elements}:
    \begin{align}
    \entropy{X} = -\tfrac{m}{n}\log_2\tfrac{m}{n} - \tfrac{n-m}{n}\log_2\tfrac{n-m}{n}
    \end{align}
    The output entropy
    \begin{align}
     & \entropy{f_{sample}(Z)} \\ 
                    =& \entropy{X,Y} \\
                    =& \entropy{X} + \entropy{Y} \\
                    =& \entropy{X} + \prob{X=0}\entropy{Y|X=0} + \notag \\
                        & \quad \prob{X=1}\entropy{Y|X=1} \\
                    =& -\tfrac{m}{n}\log_2\tfrac{m}{n} - \tfrac{n-m}{n}\log_2\tfrac{n-m}{n} + \notag \\
                        & \quad \tfrac{m}{n}\log_2{m} + \tfrac{n-m}{n}\log_2(n-m) \\
                    =& -\tfrac{m}{n}\log_2m + \tfrac{m}{n}\log_2n - \notag \\
                        & \quad \tfrac{n-m}{n}\log_2(n-m) + \notag \\
                        & \quad \tfrac{n-m}{n}\log_2{n} + \tfrac{m}{n}\log_2{m} + \notag \\
                        & \quad  \tfrac{n-m}{n}\log_2(n-m) \\
                    =& \tfrac{m}{n}\log_2n + \tfrac{n-m}{n}\log_2n \\
                    =& (\tfrac{m}{n} + \tfrac{n-m}{n})\log_2n \\
                    =& \log_2n \\
                    =& \entropy{Z}
    \end{align}
    which equals the input entropy, so therefore $f_{sample}$ preserves entropy.
\end{proof}

\begin{corollary}
    Algorithms \ref{alg:combine}, \ref{alg:divide} and \ref{alg:sample} preserve entropy, from Corollary \ref{cor:combine-entropy} and Lemma \ref{lem:sample-entropy}.
\end{corollary}

Algorithms \ref{alg:combine} and \ref{alg:sample} could be used to implement von Neumann's rejection sampling algorithm \cite{neumann51} and Lumbrusco's FDR algorithm \cite{lumbroso2013optimal}. The reason why rejection sampling and FDR lose entropy is because they discard one or both outputs from the $\textsc{sample}$ algorithm.



\subsection{Generating uniform variables}

The first step is to obtain a uniform variable $S \sim \unif{rn}$ where $n$ is the number you need to generate, and $r$ is some integer. The insight is that we can use $\textsc{sample}$ to resize $S \sim \unif{s}$ very slightly to $S \sim \unif{s - s \mod n}$ with almost no entropy loss, and the resulting size will be divisible $n$.

Algorithm \ref{alg:generate-multiple} generates a uniform variable $S \sim \unif{rn}$, by combining binary or uniform entropy into an existing store (line 4) until it reaches a minimum size $s_{min}$ (line 3). Then it resizes $S$ to be a multiple of $n$ on line 8, and if successful, returns the result on line 10. If it fails ($B=0$), then repeat. The reason this is efficient is because when $n$ is much smaller than $s_{min}$, it is overwhelmingly likely that $B=1$, and so the lost entropy in $B$ (on line 8) is very small. Algorithm \ref{alg:generate-multiple} terminates with probability 1.

Algorithm \ref{alg:generate-uniform} generates a uniform variable $N \sim \unif{n}$ using an entropy source and an entropy store $S \sim \unif{s}$. Unused entropy is returned in $S$. The algorithm calls $\textsc{generate\_multiple}$ to create a uniform variable $S \sim \unif{rn}$, then calls $\textsc{divide}$ (Algorithm \ref{alg:divide}) to split $S$ into $S \sim \unif{r}$ and $N \sim \unif{n}$. $N$ is our result, and $S$ is the new entropy store. A C implementation of Algorithm \ref{alg:generate-uniform} is given in the Appendix.

\begin{algorithm}
\caption{Generating a uniform multiple}
\label{alg:generate-multiple}
\begin{algorithmic}[1]
\Require $S, s, s_{min}, n$ and $b$ are integers
\Require $n \le s_{min}$
\Require $b \ge 2$
\Require $\textsc{fetch}() \sim \unif{b}$
\Require $S \sim \unif{s}$
\Ensure $S \sim \unif{rn}$
\Ensure $s = rn$
\Procedure{generate\_multiple}{$S, s, n$} 
  \While {True}
    \While {$s < s_{min}$}
        \State $S, s \gets \textsc{combine}(S, s, \textsc{fetch}(), b)$
    \EndWhile
    \State $r \gets s \div n$
    \State $m \gets s \mod n$
    \State $S, s, B \gets \textsc{sample}(S, s, s-m)$ 
    \If{$B=1$}
        \State \Return $S, s, r$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Generating a uniform variable of a given size}
\label{alg:generate-uniform}
\begin{algorithmic}[1]
\Require $S, s$ and $n$ are integers
\Require $S \sim \unif{s}$
\Ensure $N \sim \unif{n}$
\Ensure $S \sim \unif{s}$
\Ensure $S \indep N$
\Procedure{generate\_uniform}{$S, s, n$} 
  \State $S, s, r \gets \textsc{generate\_multiple}(S, s, n)$
  \State $S, s, N \gets \textsc{divide}(S, s, n)$
  \State \Return $S, s, N$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
    In Algorithm \ref{alg:generate-uniform}, $N \sim \unif{n}$, $S \sim \unif{s}$ and $S \indep N$.
\end{lemma}

\begin{proof}
    The preconditions of $\textsc{divide}$ on line 3 are met, because $s=rn$ implies that $s$ is divisible by $n$. Therefore the postconditions of $\textsc{divide}$ are also met, so $N \sim \unif{n}$, $S \sim \unif{s}$ and $S \indep N$.
\end{proof}

\begin{lemma}
    \label{lem:shannon-inequality}

For $p,q \in \mathbb{R}$, where $0 \le p\le q \le \frac{1}{2}$, 

\begin{align}
-p\log_2 p - (1-p)\log_2(1-p) \\ \le
-q\log_2 q - (1-q)\log_2(1-q)
\end{align}
\end{lemma}

\begin{proof}
    Let
    \begin{align}
        g(p) & = -p\log_2 p - (1-p)\log_2(1-p) \\
        \implies g'(p)
            & = \log_2\frac{1-p}{p} \\ 
            &= \log_2(\frac{1}{p}-1) \\
            &\ge \log_2(\frac{1}{\frac{1}{2}}-1) \\
            &= \log_21 \\ 
            &= 0 
    \end{align}
Since the derivative of $g\ge 0$ it means that $g$ is monotonic in the range.
\end{proof}

\begin{definition}
    Let $\epsilon = \epsilon(p)$ be the expected entropy loss function of Algorithm \ref{alg:generate-multiple}, where $p=\frac{n-1}{s_{min}}$. Let $\epsilon_{max}$ be the upper bound of $\epsilon$.
\end{definition}

\begin{theorem}
    \label{thm:loss}
If $p = \frac{n-1}{s_{min}} \le \frac{1}{2}$, then

\begin{equation}
0 \le \epsilon \le -\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{equation}

\end{theorem}

\begin{proof}
For each iteration $i$ of Algorithm \ref{alg:generate-multiple}, let $p_i = \frac{s_i \mod n}{s_i}$. But $(s_i \mod n) \le (n-1)$ and $s_i \ge s_{min}$, so $\frac{s_i \mod n}{s_i} \le \frac{n-1}{s_{min}}$, so $p_i \le p$. On each iteration, the entropy lost is equal to the entropy in the variable $B_i \sim \bern{p_i}$, which is given by the entropy equation for a Bernoulli distribution \cite{cover1999elements}:

\begin{equation}
\entropy{B_i} = -p_i\log_2p_i - (1-p_i)\log_2(1-p_i)
\end{equation}

Therefore, 

\begin{equation}
0 \le \entropy{B_i} \le -p\log_2p - (1-p)\log_2(1-p) 
\end{equation}

by Lemma \ref{lem:shannon-inequality}. The expected number of iterations $m$ is given by

\begin{align}
& m = 1 + p_im \le 1 + pm \\
\implies & m-pm \le 1 \\
\implies & m(1-p) \le 1 \\
\implies & m \le \frac{1}{1-p}
\end{align}

The total entropy $\epsilon$ lost by the algorithm is given by the number of iterations $m$ of the algorithm multiplied by the entropy $\entropy{B_i}$ lost in each iteration.

\begin{align}
\epsilon
    = & m\entropy{B_i} \\
    \le & \frac{1}{1-p}(-p\log_2p - (1-p)\log_2(1-p) ) \\
    = & -\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{align}

We can also end up in the situation where $(s \mod n) = 0$ already, so $\textsc{sample}$ succeeds with no entropy loss, so $\epsilon=0$.
\end{proof}

\begin{corollary}
    If $p = \frac{n-1}{s_{min}} \le \frac{1}{2}$, then the maximum entropy lost by Algorithm \ref{alg:generate-multiple} is
\begin{align}
    \label{eq:epsilon-max}
    \epsilon_{max} = -\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{align}
\end{corollary}

\begin{corollary}
    The maximum entropy loss of Algorithm \ref{alg:generate-uniform} is $\epsilon_{max}$.
\end{corollary}

\begin{proof}
    Algorithm \ref{alg:generate-uniform} loses up to $\epsilon_{max}$ entropy in its call to $\textsc{generate\_multiple}$ on line 2, and the $\textsc{divide}$ on line 3 does not lose entropy according to Corollary \ref{cor:combine-entropy}.
\end{proof}

\begin{corollary}
    If $p = \frac{n-1}{s_{min}} \le \frac{1}{2}$, then the minimum efficiency $\eta_{min}$ of Algorithm \ref{alg:generate-uniform} is 

\begin{align}
    \label{eq:generate-uniform-efficiency}
    \eta_{min} = \frac{\log_2n}{\log_2n + \epsilon_{max}}
\end{align}
\end{corollary}

\begin{proof}
\begin{align}
    \eta & = \frac{H_{out}}{H_{in}}
          = \frac{H_{out}}{H_{out}+\epsilon} 
          \ge \frac{\log_2n}{\log_2n + \epsilon_{max}}
\end{align}
\end{proof}

To illustrate Equation \ref{eq:generate-uniform-efficiency}, if $s_{min}=2^{31}$ and $n=6$, then $\eta \ge 0.99999997$. This means that even with a modest entropy buffer, we can get very good entropy efficiency.

\begin{corollary}
The entropy efficiency $\eta$ of Algorithm \ref{alg:generate-uniform} is arbitrarily close to 1.
\end{corollary}

\begin{proof}
$\epsilon(\frac{n-1}{s_{min}}) \rightarrow 0$ as $s_{min} \rightarrow \infty$, using L'H\^opital's Rule on Equation \ref{eq:epsilon-max}. Therefore $\eta \rightarrow 1$ as $s_{min} \rightarrow \infty$.
\end{proof}



\subsection{Generating Bernoulli variables}

Algorithm \ref{alg:generate-bernoulli} generates a Bernoulli variable $B \sim \unif{\frac{m}{n}}$ using an entropy source and an entropy store $S \sim \unif{s}$. Unused entropy is returned in $S$. The call to $\textsc{generate\_multiple}$ ensures that the store $S \sim \unif{rn}$ so we can apply $\textsc{sample}$ (Algorithm \ref{alg:sample}) to extract $S \sim \unif{rm}$ or $S \sim \unif{n-rm}$ from $S$. We return the Bernoulli output $B$ and recycle the remaining $S$ as the new entropy store.

\begin{algorithm}
\caption{Generating a Bernoulli variable}
\label{alg:generate-bernoulli}
\begin{algorithmic}[1]
\Require $S, s, m, n$ are integers
\Require $S \sim \unif{s}$
\Require $0 \le m \le n\le s_{min}$
\Ensure $B \sim \bern{\frac{m}{n}}$
\Ensure $S \sim \unif{s}$
\Ensure $S \indep B$
\Procedure{generate\_bernoulli}{$S, s, m, n$} 
  \State $S, s, r \gets \textsc{generate\_multiple}(S, s, n)$
  \State $S, s, B \gets \textsc{sample}(S, s, r * m)$
  \State \Return $S, s, B$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
Algorithm \ref{alg:generate-bernoulli} returns $S \sim \unif{s}$ and $B \sim \bern{\frac{m}{n}}$. $B \indep S$.
\end{lemma}

\begin{proof}
    $\textsc{generate\_multiple}$ creates $S \sim \unif{rn}$ on line 2, then $\textsc{sample}$ returns $B \sim \bern{\frac{rm}{rn}} = \bern{\frac{m}{n}}$, and $S \sim \unif{s}$. By Lemma \ref{lem:sample}, $B \indep S$.
\end{proof}

\begin{lemma}
The entropy loss of Algorithm \ref{alg:generate-bernoulli} is given by $\epsilon(\frac{n-1}{s_{min}})$.
\end{lemma}

\begin{proof}
    The only entropy loss from Algorithm \ref{alg:generate-bernoulli} is the call to $\textsc{generate\_multiple}$ on line 2, which loses $\epsilon(\frac{n-1}{s_{min}})$ according to Theorem \ref{thm:loss}.
\end{proof}

\subsection{Generating weighted integer variables}

Algorithm \ref{alg:generate-weighted} generates a weighted variable $W \sim \mathrm{Weighted}\{w_0, ..., w_{k-1}\}$ using an entropy source and an entropy store $S \sim \unif{s}$. Unused entropy is returned in $S$. The distribution has $k$ outcomes where each outcome has integer weight $\{w_0, w_1, ..., w_{k-1}\}$, giving output probabilities $\{\frac{w_0}{n}, \frac{w_1}{n}, ..., \frac{w_{k-1}}{n}\}$, where $n=\sum_i w_i$ is the total weight.

The algorithm works by mapping the $n$ outcomes of a uniform distribution to the $k$ outputs of the weighted distribution using a lookup table. An implementation of this could be an array of size $n$ (constuction time $O(n)$, lookup time $O(1)$), or a binary search on an array of size $k$ (construction time $O(k)$, lookup time $O(\log k)$).

We can divide the interval $[0,n)$ into $k$ intervals with size $w_i$ as follows:

\[
\overbrace{
    \underbrace{
        \underbrace{z_0 ... z_{w_0-1}}_{Y_0 \sim \unif{w_0}}
          \underbrace{z_{w_0} ... z_{w_0+w_1-1}}_{Y_1 \sim \unif{w_1}}
          ...
          \underbrace{
             z_{n-w_{k-1}} ... 
             z_{n-1}
         }_{Y_{k-1} \sim \unif{w_{k-1}}}
    }_{X \sim \mathrm{Weighted}\{w_0, ..., w_{k-1}\}}
}^{Z \sim \unif{n}}
\]

When we map $Z$ to the output variable $X$, then we can also produce a uniform variable $Y \sim \unif{w_i}$ which contains $\log_2w_i$ entropy, which we can recycle into the entropy store. Lemma \ref{lem:distribution-conservation} shows us that when we account for this extra entropy then no entropy is lost.

\begin{definition}
    \label{def:o}
    For integer weights $w_i \ge 0$, $0 \le i < k$, let the offset $o_0$ = 0, $o_{i+1} = o_i + w_i$.
\end{definition}

\begin{definition}
    \label{def:t}
    Let the output $t_j = i$ for $o_i \le j < o_{i+1}.$
\end{definition}

\begin{definition}
    Define $f_{weighted}: Z \mapsto (t_Z, Z-o_X)$.
\end{definition}

\begin{lemma}
    If $Z \sim \unif{n}$ and $f_{weighted}: Z \mapsto (X, Y)$, then $X \sim \mathrm{Weighted}\{w_0, ...\}$ and $Y \sim \unif{w_X}$.
\end{lemma}

\begin{proof}
    $\prob{X=x} = \prob{X = t_Z} = \frac{o_{x+1} - o_x}{n} = \frac{w_x}{n}$. Therefore $X \sim \mathrm{Weighted}\{w_0 ...\}$.  $\prob{Y=y} = \prob{Y = Z - o_X | X = x} = \prob{Y = Z - o_x | o_{x} \le Z < o_{x+1}} = \frac{1}{o_{x+1}-o_x} = \frac{1}{w_x}$. Therefore $Y \sim \unif{w_X}$ and $X \indep Y$.
\end{proof}

$f_{weighted}$ is a generalisation of $f_{combine}$ and $f_{sample}$. Although $X$ is independent of $Y$, the \em size \em of $Y$ is not independent of $X$.

\begin{lemma}
    \label{lem:distribution-conservation}
    $f_{weighted}$ does not lose entropy.
\end{lemma}

\begin{proof}
    Let $q_i = \frac{w_i}{n} = \prob{X=i}$.
    \begin{align}
    & \entropy{f_{weighted}(Z)} \notag \\
               = & \entropy{X,Y} \\
               = & \entropy{X} + \entropy{Y} \\
                = & - \sum_i q_i \log_2q_i + \notag \\ 
                    & \quad \sum_i \prob{X=i}\entropy{Y|X=i} \\
                = & - \sum_i q_i \log_2 \frac{w_i}{n} + \sum_i q_i\log_2 w_i \\
                = & - \sum_i q_i(\log_2 w_i - \log_2 n) + \notag \\ 
                    & \quad \sum_i q_i\log_2 w_i \\
                = & - \sum_i q_i\log_2 w_i + \sum_i q_i \log_2 n + \notag \\ 
                    & \quad\sum_i q_i\log_2 w_i \\
                = & \sum_i q_i \log_2 n \\
                = & (\log_2 n) \sum_i q_i \\
                = & \log_2 n \\
                = & \entropy{Z}
    \end{align}
    Therefore the entropy of the input and outputs of $f_{weighted}$ is the same, and $f_{weighted}$ does not lose any entropy.
\end{proof}

Algorithm \ref{alg:generate-weighted} generates a weighted variable by first calling $\textsc{generate\_multiple}$ to generate $S \sim \unif{rn}$. Lines 3-5 implement $f_{weighted}$ to yield the weighted variable $W$ and a uniform variable $S \sim \unif{rw_W}$. The weights have been scaled by a factor of $r$ to avoid recomputing the lookup tables.

\begin{algorithm}
\caption{Generating a weighted variable}
\label{alg:generate-weighted}
\begin{algorithmic}[1]
\Require $S, s$ and $n$ are integers
\Require An array $w$ of $k$ integer weights $w_i \ge 0$ totalling $n$
\Require An array $t$ of $n$ integer outputs (see Definition \ref{def:t})
\Require An array $o$ of $k$ integer offsets (see Definition \ref{def:o})
\Require $S \sim \unif{s}$
\Ensure $S \sim \unif{s}$
\Ensure $W \sim \mathrm{Weighted}\{w_0, ...\}$
\Ensure $S \indep W$
\Procedure{gen\_weighted}{$S, s, n, w, t, o$} 
    \State $S, s, r \gets \textsc{generate\_multiple}(S, s, n)$
    \State $W \gets t_{[S \div r]}$
    \State $s \gets r * w_{[W]}$
    \State $S \gets S - r * o_{[W]}$
    \State \Return $S, s, W$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
    The entropy loss of Algorithm \ref{alg:generate-weighted} is given by $\epsilon(\frac{n-1}{s_{min}})$.
\end{lemma}

\begin{proof}
    By Lemma \ref{lem:distribution-conservation}, the only entropy-losing step in Algorithm \ref{alg:generate-weighted} is the call to $\textsc{generate\_multiple}$, which loses $\epsilon(\frac{n-1}{s_{min}})$ entropy by Theorem \ref{thm:loss}.
\end{proof}

\section {Evaluation}

We can evaluate a random number generator based on its speed, memory footprint, setup time, entropy efficiency and quality of output distribution. \cite{saad2025}

For entropy efficiency, we can compare entropy store against entropy-optimal algorithms Knuth-Yao (KY) and the Interval Algorithm (IA).

Figure \ref{fig:uniform-losses} shows the calculated entropy loss for generating uniform variables, comparing ES with von Neumann (vN) and KY generators. As expected (see for example \cite{bacher2017} for a detailed analysis), the entropy loss of vN and KY generators depend on the number generated, with KY losing up to 2 bits per output.

This figure shows the entropy loss calculated using Equation \ref{eq:epsilon-max} for a 32-bit and a 64-bit entropy buffer, with an increase in buffer size yielding a higher efficiency. We also show sample real-world entropy losses confirming that $\epsilon_{max}$ is an upper bound.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{uniform_losses.png}
\caption{Calculated entropy losses generating $\unif{n}$ for different algorithms.}
\label{fig:uniform-losses}
\end{figure}

Figure \ref{fig:shuffling-efficiency} shows the overall impact of entropy losses when applied to the shuffling a deck of $N$ cards using the Fisher-Yates algorithm \cite{durstenfeld1964algorithm, fisher1953statistical, knuth2014art}. This is a good demonstration of the entropy efficiency under a real-world workload. The total entropy loss of ES is $\sum_{n=2}^N \epsilon(\frac{n-1}{s_{min}})$, so with a 32-bit buffer, shuffling a deck of 52 cards can be done with an entropy loss of $\le 1.7 \times 10^{-5}$ bits, or an entropy efficiency of $\ge 0.99999992$.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{shuffling_efficiency.png}
\caption{Calculated entropy efficiency shuffling cards using the Fisher-Yates algorithm, using different uniform generator algorithms.}
\label{fig:shuffling-efficiency}
\end{figure}

When generating Bernoulli variables, we see in Figure \ref{fig:bernoulli-efficiency} that the entropy efficiency of IA drops off significantly at lower entropy. IA has spikes at the dyadic points that are more efficient to generate. IA must fetch between 1 and 2 bits per output on average. By contrast, ES algorithms do not necessarily fetch any bits to generate an output, as there may be enough entropy in the store already, so ES just shrinks the size of its store to generate an output. Figure \ref{fig:bernoulli-rate} shows that IA cannot significantly increase its output rate for low-entropy outputs.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{bernoulli_efficiency.png}
\caption{Calculated entropy efficiency generating Bernoulli variables for different algorithms.}
\label{fig:bernoulli-efficiency}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{bernoulli_rate.png}
\caption{Output rate for Bernoulli distribution $\bern{\frac{1}{n}}$ for different algorithms.}
\label{fig:bernoulli-rate}
\end{figure}

For practical purposes, Algorithms \ref{alg:generate-uniform}, \ref{alg:generate-bernoulli} and \ref{alg:generate-weighted} are $O(1)$.
The setup for Algorithms \ref{alg:generate-uniform} and \ref{alg:generate-bernoulli} are $O(1)$, but Algorithm \ref{alg:generate-weighted} requires $O(n)$ setup time and space. Integer division and multiplication is actually $O(m \log m)$ \cite{harvey2021integer}, so for an $m$ bit ES the time per output is $O(m \log m)$. 

Table~\ref{tab:speed} compares the speed of ES against various best in class random number generators. These numbers are heavily compiler and CPU dependent, so are only a guide.

When reading entropy from a hardware random generator, rate of entropy input is the dominating factor and the ES is the fastest by a significant margin. When using a very fast pseudo-random source like Xorisho-128 \cite{blackman21}, we see that Lemire's algorithm \cite{lemire2019fast} dominates, as this is designed for speed over entropy efficiency.

Speed is best understood in terms of CPU pipeline architecture. Branching, memory access, hardware random numbers and integer divmod operations are all relatively expensive operations \cite{Abel19a}. Table-based methods like FLDR \cite{saad2020fldr} and ALDR \cite{saad2025} perform memory access which can be slower than purely arithmetic methods. ES is likely to suffer due to its 2 integer divmod operations, but compilers can strength-reduce integer division to a multiplication and a shift \cite{granlund94}, which is shown in the "optimized" version of these algorithms in Table~\ref{tab:speed}. ES was the fastest for Bernoulli outputs in all cases because there is only one divmod operation needed per output.

Overall, ES is most suited when generating true random variables from a hardware entropy source. When a pseudo-random source is used, then Lemire's algorithm is the fastest. Although a 64-bit ES is more efficient that a 32-bit ES, the slower 64-bit operations outweigh the marginal improvement in efficiency.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Algorithm & Output & H/W & Xoshiro \\
\hline
ES32                  & $\unif{6}$ & \textbf{1} & 1 \\
ES32 optimized        &  & \textbf{1} & 0.47 \\
ES64                  & & \textbf{1} & 1.24 \\
ES64 optimized        &  & \textbf{1} & 0.55 \\
vN \cite{neumann51}     &  & 1.55 & 0.77 \\
FDR \cite{lumbroso2013optimal} &  & 1.42 & 0.83 \\
Huber-Vergas \cite{huber2024optimalrollingfairdice} &  & 1.42 & 1.0 \\
FLDR \cite{saad2020fldr} &  & 1.42 & 1.46 \\
ALDR \cite{saad2025} &  & 1.42 & 1.5 \\
Lemire \cite{lemire2019fast} & & 24.8 & \textbf{0.26} \\
\hline

ES32                  & $\bern{\frac{1}{100}}$ & 1 & 1 \\
ES32 optimized        &  & \textbf{0.9} & \textbf{0.54} \\
FLDR                  &  & 28.3 & 3.8 \\
ALDR                  &  & 22.4 & 2.94 \\

\hline

ES32                  & $\mathrm{Weighted}$ & \textbf{1} & \textbf{1} \\
FLDR                  & \{1,2,3,4,5\} & 1.36 & 1.38 \\
ALDR                  & & 1.36 & 1.13 \\

\hline

\end{tabular}
\caption{Relative times generating variables from hardware entropy (H/W) and Xoshiro-128 \cite{blackman21} (lower is better).}
    \label{tab:speed}
\end{table}



\section{Conclusion and further work}

We have introduced a new class of algorithm to generate random variables using a uniform entropy store, and shown that by recycling unused entropy between outputs we can achieve arbitrarily low entropy losses. We have shown that the entropy efficiency limits of classical algorithms like Knuth-Yao \cite{Knuth1976TheCO} and the Interval Algorithm \cite{han97} can be overcome by using an entropy store. 

A big benefit of ES is that no setup step is needed for uniform and Bernoulli outputs, and that the same ES can be shared between different output types. ES is fastest when generating outputs from a hardware entropy source and for Bernoulli outputs, but is still competitive on pseudorandom entropy sources, particularly if the division operations can be optimized.

Further work could include improving the speed, generating other distributions, and determining if the current algorithms are optimal or if further efficiency improvements are possible for a given store size.



\printbibliography

\appendix

Source code for $\textsc{generate\_uniform}$, written in C.

\begin{verbatim}
  const uint32_t s_min = 1<<31;
  uint32_t s_value = 0;
  uint32_t s_range = 1;

  uint32_t
  generate_uniform32(uint32_t n)
  {
    for(;;)
    {
      // Preload entropy one bit 
      // at a time into s
      while(s_range < s_min)
      {
        s_value <<= 1;
        s_value |= fetch();
        s_range <<= 1;
      }
      // Sample entropy s to a
      // multiple of n
      uint32_t r = s_range / n;
      uint32_t m = s_range % n;
      if(s_value >= m)
      {
        // Sample successful
        s_value -= m;
        uint32_t a = s_value / n;
        uint32_t b = s_value % n;
        s_value = a;
        s_range = r; 
        return b;
      }
      else
      {
        // Reject
        s_range = m;
      }
    }
  }
\end{verbatim}

\end{document}
