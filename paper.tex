\documentclass[12pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}   % Math
\usepackage{graphicx}           % Images
% \usepackage{hyperref}           % Clickable links
\usepackage{geometry}           % Page margins
% \usepackage{cite}               % Citation formatting
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% BibLaTeX for references (requires biber)
\usepackage[
    backend=biber,
    style=numeric,
    sorting=nyt
]{biblatex}

\addbibresource{references.bib}  % Bib file

% Page setup
\geometry{margin=1in}

% Title
\title{Algorithms for efficient entropy conversion}
\author{Calum Grant \\
OxFORD Asset Management \\
calum.grant@oxam.com}
\date{\today}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\lstset{
  language=C,        % choose the language
  basicstyle=\ttfamily\small, % font style and size
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{orange},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  breaklines=true,
  frame=single,
  captionpos=b
}

\begin{document}

\maketitle

\begin{abstract}
    We present practical and efficient algorithms for generating random distributions from binary entropy, using an entropy store. This minimises entropy loss, for example we can shuffle a deck of 52 cards using just $\approxeq 225.58102$ bits of entropy, yielding an entropy conversion efficiency of $>0.99999992$ using a 32-bit entropy cache, compared with state of the art methods that only have a $\approxeq 0.81$ efficiency.  The algorithm requires two 32-bit integer divmod operations per random integer generated, making it significantly slower than other algorithms.
\end{abstract}

\section{Introduction}

Generating uniform random integers has many applications, and there are scenarios where we want random integers that are perfectly random and perfectly distributed. True randomness comes from an external source, often delivered as a string of random bits. The external entropy must be converted to a different base that is not a power of 2, whilst consuming as few random bits as possible.

One of the first algorithms for generating uniformly distributed integers was developed by von Neumann \cite{neumann}, known as \em rejection sampling \em, and is still used to this day, for example in []. The rejection sampling algorithm, shown in Algorithm \ref{alg:rejection-sampling}, fetches random bits, either from hardware or from a pseudo-random number generator, to give a uniform integer distribution of a power of 2. Then check if the number is less than the target range n. If the number is less than n, return it, otherwise try again. This algorithm terminates with probability 1 and can be shown to generate perfectly uniform random integers.

\begin{algorithm}
\caption{Generating uniform integers using rejection sampling}
    \label{alg:rejection-sampling}
\begin{algorithmic}[1]
    \Require $n$ is an integer $>0$
    \Require $fetch()$ returns an unbounded stream of unbiassed bits
    \Ensure $v$ is uniformly distributed over $[0,n)$
\Procedure{rejection\_sampling}{$n$}
    \While{true}
        \State $r \gets 1$
        \State $v \gets 0$
        \While {$r < n$}
            \State $r \gets r * 2$
            \State $v \gets v * 2 + fetch()$
        \EndWhile
        \If {$v<n$}
            \State \Return $v$
        \EndIf
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Generating uniform integers with Fast Dice Roller}
    \label{alg:fast-dice-roller}
\begin{algorithmic}[1]
    \Ensure $v$ is uniformly distributed over $[0,n)$
    \Procedure{fast\_dice\_roller}{$n$}
    \State $r \gets 1$
    \State $v \gets 0$
    \Loop
        \While {$r < n$}
            \State $r \gets r * 2$
            \State $v \gets v * 2 + fetch()$
        \EndWhile
        \If {$v<n$}
            \State \Return $v$
        \EndIf
        \State $r \gets r-n$
        \State $v \gets v-n$
    \EndLoop
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Related work}


\section{Algorithms for entropy conversion}

In this section we'll start with some basic operations (Algorithm \ref{alg:combine}-\ref{alg:upsample}), then use these to create an efficient generator for uniform integers (Algorithm \ref{alg:generate-uniform}). We'll then build on Algorithm \ref{alg:generate-uniform} to create other types of entropy converters, such as converting between arbitrary uniform distributions in Algorithm \ref{alg:convert_u_u}, generate Bernoulli distributions efficiently using Algorithm \ref{alg:generate-bernoulli}, combine uniform and Bernoulli distributions using Algorithm \ref{alg:consume_b} and efficiently convert between uniform and Bernoulli distributions using Algorithm \ref{alg:convert_b_b}.

These algorithms are engineered to minimise entropy loss, and the analysis of their entropy efficiency will come in the following section.

Algorithm \ref{alg:combine} combines entropy in two uniform distrete random variables into a single uniform discrete random variable.

\begin{algorithm}
\caption{Combining uniformly distributed integers}
\label{alg:combine}
\begin{algorithmic}[1]
    \Require $n$, $m$, $U_n$, $U_m$ are integers
    \Require $n>0$, $m>0$
    \Require $U_n$ is uniformly distributed over $[0,n)$
    \Require $U_m$ is uniformly distributed over $[0,m)$
    \Ensure $nm$ is $n * m$
    \Ensure $U_{nm}$ is uniformly distributed over $[0,nm)$
\Procedure{combine}{$U_n, n, U_m, m$} 
  \State $U_{nm} \gets U_n * m + U_m$
  \State $nm \gets n * m$
  \State \Return $U_{nm}, nm$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:combine}, $U_{nm}$ is uniformly distributed over $[0,nm)$.
\label{lem:combine}
\end{lemma}

\begin{proof}
Let $X \sim Uniform \{0 ... n-1\}$ and $Y \sim Uniform\{0 ... m-1\}$ be independent uniformly distributed random variables. The joint distribution $(X,Y)$ is uniformly distributed with $nm$ elements of probability $\frac{1}{nm}$. Let $Z$ be the distribution defined as

\begin{equation}
Z = f(X,Y) = mX+Y
\end{equation}

The mapping $f$ is a bijection between the pair $X \times Y$ and $Z$, so $Z$ is also uniformly distributed and 

\begin{equation}
Z \sim Uniform \{0 ... nm-1\}
\end{equation}
\end{proof}

Algorithm \ref{alg:divide} is the inverse of Algorithm \ref{alg:combine}, allowing us to factorise a uniformly distributed integer into two. For this, the sizes of the output distributions must divide the size of the input distribution.

\begin{algorithm}
\caption{Division of uniformly distributed integers}
\label{alg:divide}
\begin{algorithmic}[1]
    \Require $nm$, $n$, $U_{nm}$ are integers
    \Require $nm>0$, $m>0$
    \Require $nm$ is divisible by $n$
    \Require $U_{mn}$ is uniformly distributed over $[0,nm)$
    \Ensure $n * m = nm$
    \Ensure $U_{n}$ is uniformly distributed over $[0,n)$
    \Ensure $U_{m}$ is uniformly distributed over $[0,m)$
    \Ensure $U_n$ and $U_m$ are independent
\Procedure{divide}{$U_{nm}, mn, n$} 
  \State $U_m \gets U_{nm} \operatorname{div} n$
  \State $U_{n} \gets U_{nm} \mod n$
  \State $m \gets nm / n$
  \State \Return $U_n, U_m, m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:divide}, $U_n$ is uniformly distributed over $[0,n)$ and $U_m$ is uniformly distributed over $[0,m)$. $U_m$ and $U_n$ are independent.

\label{lem:divide}
\end{lemma}

\begin{proof} $f$ is a bijection so has an inverse function 
\begin{equation}    
X = \lfloor Z/n \rfloor, Y = Z \mod n
\end{equation}

Since $X$ and $Y$ are the original random variables, they are independent and uniformly distributed.
\end{proof}

Algorithm \ref{alg:downsample} converts a uniformly distributed integer to a smaller range, and also returns a biassed bit. Unlike Algorithm \ref{alg:combine} and Algorithm \ref{alg:divide}, the size of the output distribution depends on the value. The biassed bit also contains entropy, and in fact the total entropy returned by this algorithm is the same as its input entropy.

\begin{algorithm}
\caption{Downsampling uniformly distributed integers}
\label{alg:downsample}
\begin{algorithmic}[1]
    \Require $U_{n}$, $m$ and $n$ are integers 
    \Require $0 \le m \le n$
    \Require $U_{n}$ is uniformly distributed over $[0,n)$
\Ensure $U_{x}$ is uniformly distributed over $[0,x)$
\Ensure $x = m$ or $x=n-m$
\Ensure $B$ is a Boolean value Bernoulli distributed with $p=\frac{m}{n}$
\Ensure $U_x$ and $B$ are independent
\Procedure{downsample}{$U_n, n, m$} 
  \If{$U_n < n$}
    \State $B \gets True$  
    \State $x \gets m$
    \State $U_x \gets U_n$
  \Else
    \State $B \gets False$  
    \State $x \gets m-n$
    \State $U_x \gets U_n-m$
  \EndIf
  \State \Return $U_x, x, B$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:downsample}, $U_x$ is uniformly distributed over $[0,x)$.
\label{lem:downsample}
\end{lemma}

\begin{proof}
    Let $y = U_n$.
    $P(y<m) = \frac{m}{n}$ so $B \sim Bernoulli\{\frac{m}{n}\}$.

If $y < m$, then $y$ is uniformly distributed between $[0,m)$.

If $y \ge m$, then $y$ is uniformly distributed between $[m, n)$, so $y-m$ is uniformly distributed between $[0, n-m)$.
\end{proof}

Algorithm \ref{alg:upsample} increases the size of a uniform distribution by incorporating Bernoulli entropy. This could be used to harness entropy from biassed Bernoulli sources.

\begin{algorithm}
\caption{Upsampling uniformly distributed integers}
\label{alg:upsample}
\begin{algorithmic}[1]
\Require $U_x$ is uniformly distributed over $[0,x)$
\Require $B$ is a Boolean value Bernoulli distributed with $p=\frac{m}{n}$
\Require $x=m$ if $B$ else $x=n-m$
\Ensure $U_n$ is uniformly distributed over $[0,n)$.
\Procedure{upsample}{$U_x, x, n, B$} 
  \If{$B$}
    \State $U_n \gets U_x$  
  \Else
    \State $U_n \gets n-x+U_x$  
  \EndIf
  \State \Return $U_n$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:upsample}, $U_{n}$ is uniformly distributed over $[0,n)$.
\end{lemma}

\begin{proof}
Let $x = U_{n} \sim X$. The possible values of $X$ are $\{0 ... n-1\}$.

Case 1: $x<n$

\begin{align}
P(x=X) & = P(B)P(x=X|B) + P(\neg B)P(x=X|\neg B) \\
       & = \frac{m}{n}\frac{1}{m} + 0 \\
       & = \frac{1}{n}
\end{align}

Case 2: $x \ge n$

\begin{align}
P(x=X) & = P(B)P(x=X|B) + P(\neg B)P(x=X|\neg B) \\
       & = 0 + (1 - \frac{m}{n})\frac{1}{n-m}  \\
       & = \frac{n - m}{n}\frac{1}{n-m} \\
       & = \frac{1}{n}
\end{align}

This means that $x \sim Uniform\{0...n-1\}$.
\end{proof}

Algorithm \ref{alg:generate-uniform} reads binary entropy from a $fetch()$ function, and outputs a uniform integer in the range $[0,n)$. The algorithm makes use of an \em entropy store \em $U_s$ which is carried over in between function calls. In a practial implementation, $U_s$ and $s$ can be captured variables or class members. Initially the entropy store is empty (containing $0$ entropy) with $U_s = 0$ and $s=1$.

The overall strategy of Algorithm \ref{alg:generate-uniform} is to use $downsample$ (Algorithm \ref{alg:downsample}) to ensure that $s$ is a multiple of $n$, then use the $divide$ algorithm (Algorithm \ref{alg:divide}) to divide $U_s$ into $U_n$. $U_n$ is returned as the result, and $U_s$ which is stored for the next invocation. The calculation is structured so that when $s$ is large, the entropy lost by $downsample$ is very small.

The $downsample$ on line 6 resizes $s$ to a multiple of $n$. It is overwhelmingly likely that $b$ is false because $n$ is much smaller than $s$, so we can then proceed to line 8 where we divide $U_s$ into $U_n$ and the new $U_s$. On line 9, return $U_n$ as the result and $U_s$ and $s$ as the input to the next invocation.

An C implemetation of Algorithm \ref{alg:generate-uniform} is given in Appendix \ref{adx:source}.

\begin{algorithm}
\caption{Generating uniformly distributed integers}
\label{alg:generate-uniform}
\begin{algorithmic}[1]
\Require Integers $0 < n\le N$
\Require $fetch()$ returns Bernoulli entropy with $p=0.5$
\Require $U_s$ is uniformly distributed over $[0,s)$
\Ensure $U_n$ is uniformly distributed over $[0,n)$
\Ensure $U_s$ is uniformly distributed over $[0,s)$
\Procedure{generate\_uniform}{$U_s, s, n, N$} 
  \While {True}
    \While {$s < N$}
        \State $U_s, s \gets combine(U_s, s, fetch(), 2)$
    \EndWhile
    \State $U_s, s, b \gets downsample(U_s, s, s \mod n)$ 
    \If{$ \neg b$}
        \State $U_n, U_s, s \gets divide(U_s, s, n)$
        \State \Return $U_s, s, U_n$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
    In Algorithm \ref{alg:generate-uniform}, 
$U_n$ is uniformly distributed over $[0,n)$ and 
$U_s$ is uniformly distributed over $[0,s)$.
\end{lemma}

\begin{proof}
The values $U_n$, $n$, $U_s$ and $s$ have been generated by Algorithms \ref{alg:combine}, \ref{alg:divide} and \ref{alg:downsample}. By Lemmas \ref{lem:combine}, \ref{lem:divide} and \ref{lem:downsample}, $U_n$ and $U_s$ are uniformly distributed.
\end{proof}

\begin{lemma}
Algorithm \ref{alg:generate-uniform} terminates with probability 1.
\end{lemma}

\begin{proof}
    If $p$ be the probability that the algorithm loops, where $p<1$, then the probability $q$ that Algorithm \ref{alg:generate-uniform} loops forever is given by $q = (1-p)q \implies q=0$.
\end{proof}

Algorithm \ref{alg:generate-distribution} shows how we can generate an arbitrary  distribution of $k$ outcomes where each outcome has integer weight ${w_1 ... w_k}$, normalised to a discrete distribution with probabilities $\{\frac{w_1}{n}, \frac{w_2}{n} ... \frac{w_k}{n}\}$ where $n$ is the total weight.

The algorithm works by construct a mapping from the $n$ outcomes of a uniform distribution to the $k$ outputs of the weighted distribution. This is a simple lookup table of size $n$.

To generate an output, call $generate\_uniform$ to generate a integer in the range $[0,n)$, and use the number in the lookup table as the result. The novelty is that the resampling operation also generates a uniform distribution which also contains $\log_2w_i$ entropy. In order to be entropy efficient, we must combine this entropy back into the entropy store, which is done in the $combine$ step.

By reclaiming this extra entropy, the entropy loss from Algorithm \ref{alg:generate-distribution} is only caused by the $generate\_uniform$ step, and the entropy loss from this algorithm is $H_{loss}(n)$.

\begin{algorithm}
\caption{Constructing the lookup tables for a distribution}
\label{alg:generate-lookup-tables}
\begin{algorithmic}[1]
\Require Integers $0 < n\le N$
\Require $fetch()$ returns Bernoulli entropy with $p=0.5$
\Require $U_s$ is uniformly distributed over $[0,s)$
\Ensure $U_n$ is uniformly distributed over $[0,n)$
\Ensure $U_s$ is uniformly distributed over $[0,s)$
\Procedure{make\_distribution}{$weights$} 
  \While {True}
    \While {$s < N$}
        \State $U_s, s \gets combine(U_s, s, fetch(), 2)$
    \EndWhile
    \State $U_s, s, b \gets downsample(U_s, s, s \mod n)$ 
    \If{$ \neg b$}
        \State $U_n, U_s, s \gets divide(U_s, s, n)$
        \State \Return $U_s, s, U_n$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Generating arbitrary distributions}
\label{alg:generate-distribution}
\begin{algorithmic}[1]
\Require Integers $0 < n\le N$
\Require $fetch()$ returns Bernoulli entropy with $p=0.5$
\Require $U_s$ is uniformly distributed over $[0,s)$
\Ensure $U_n$ is uniformly distributed over $[0,n)$
\Ensure $U_s$ is uniformly distributed over $[0,s)$
\Procedure{generate\_uniform}{$U_s, s, n, N$} 
  \While {True}
    \While {$s < N$}
        \State $U_s, s \gets combine(U_s, s, fetch(), 2)$
    \EndWhile
    \State $U_s, s, b \gets downsample(U_s, s, s \mod n)$ 
    \If{$ \neg b$}
        \State $U_n, U_s, s \gets divide(U_s, s, n)$
        \State \Return $U_s, s, U_n$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}


Algorithm \ref{alg:convert_u_u} shows how we can efficiently \em convert \em entropy from one uniform distribution to another, via an entropy store. This is just a two-step process, where the first \em combine \em step is used to add the entropy from $U_n$ into $U_s$ before we generate a number $U_m$ from the same store. We'll ignore the potential overflow of $s$.

\begin{algorithm}
\caption{Converting uniform integers}
\label{alg:convert_u_u}
\begin{algorithmic}[1]
    \Require Integers $0 < m \le N$, $U_s$, $s$
    \Require $U_s$ is uniformly distributed over $[0,s)$
    \Require $U_m$ is uniformly distributed over $[0,m)$
    \Ensure  $U_s$ is uniformly distributed over $[0,s)$
    \Ensure  $U_n$ is uniformly distributed over $[0,n)$
\Procedure{convert\_uniform}{$U_s, s, U_m, m, n, N$} 
    \State $U_s, s \gets combine(U_s, s, U_m, m)$
    \State $U_s, s, U_m \gets generate\_uniform(U_s, s, n, N)$
    \State \Return $U_s, s, U_m, m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:generate-bernoulli} can be used to generate biassed bits of any rational $p=\frac{m}{n}$ value. Firstly it obtains a uniform integer $U_n$ in the range $[0,n)$, then uses \em downsample \em which compares $U_n<m$ to generate a Bernoulli distribution $B$. The remaining uniform entropy $U_x$ is added back to the entropy store using \em combine \em so that this entropy can be reused.

\begin{algorithm}
\caption{Generating biassed bits}
\label{alg:generate-bernoulli}
\begin{algorithmic}[1]
\Require Integers $0 < n \le m \le N$
\Require $U_s$ is uniformly distributed over $[0,s)$
\Ensure $U_s$ is uniformly distributed over $[0,s)$
\Ensure $B$ has a Bernoulli distribution with $p = \frac{m}{n}$.
\Procedure{generate\_bit}{$U_s, s, m, n, N$} 
    \State $U_s, s, U_n \gets generate\_uniform(U_s, s, n, N)$
    \State $U_x, x, B \gets downsample(U_n, n, m)$
    \State $U_s, s \gets combine(U_s, s, U_x, x)$
    \State \Return $U_s, s, B$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:consume_b} can be used to read biassed bits and convert them to a uniformly distributed variable. This allows us to exploit biassed entropy sources. For a bit with bias $\frac{m}{n}$, we first generate a uniform variable of size $m$ or $n-m$, then call \em upsample \em to turn this into a uniform variable of size $n$. Then we \em combine \em to combine the entropy in $U_s$ with $U_n$.

\begin{algorithm}
\caption{Consuming biassed bits}
\label{alg:consume_b}
\begin{algorithmic}[1]
\Procedure{consume\_bit}{$U_s, s, B, m, n, N$} 
    \If {B}
        \State $x \gets m$
    \Else
        \State $x \gets n-m$
    \EndIf
    \State $U_x, U_s, s \gets generate\_uniform(U_s, s, x, N)$
    \State $U_n, n \gets upsample(U_x, x, n, B)$
    \State $U_s, s \gets combine(U_s, s, U_x, x)$
    \State \Return $U_s, s$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:convert_b_b} can be used to convert between Bernoulli distributions. In isolation, it makes no sense to convert a bit with bias $\frac{1}{4}$ to a bit with bias $\frac{4}{5}$, but via an entropy store it is possible. Algorithm \ref{alg:convert_b_b} has two parts: firstly read the input into the entropy store, then generate a biassed bit from the entropy store. It is possible to output more entropy than input, in which case the additional entropy comes from the $fetch$ function embedded within $generate\_uniform$. On the other hand, if the input entropy is larger, then the excess is saved in the entropy store.

\begin{algorithm}
\caption{Converting biassed bits}
\label{alg:convert_b_b}
\begin{algorithmic}[1]
    \Require Integers ...
    \Require $U_s$ is uniformly distributed over $[0,s)$
    \Require $B_1$ is Bernoulli distributed with $p=\frac{m1}{n1}$
    \Ensure $U_s$ is uniformly distributed over $[0,s)$
    \Ensure $B_2$ is Bernoulli distributed with $p=\frac{m2}{n2}$
\Procedure{convert\_bit}{$U_s, s, B_1, n1, m1, n2, m2, N$}
    \State $U_s, s = consume\_bit(U_s, s, n1, m1, B, N)$
    \State $U_s, s, B_2 = generate\_bit(U_s, s, n2, m2, N)$
    \State \Return $U_s, s, B_2$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}

\caption{Generating arbitrary distributions}
\label{alg:generate_discrete}
\begin{algorithmic}[1]
    \Require Integers ...
    \Require $d$ is a list of integers representing weighted outcomes
    \Require $U_s$ is uniformly distributed over $[0,s)$
    \Ensure $U_s$ is uniformly distributed over $[0,s)$
    \Ensure $X$ is distributed with $P(x=X) = d_x/n$
\Procedure{generate\_distribution}{$U_s, s, d, N$}
    \State $n \gets \sum dist$
    \State $U_n, U_s, s = generate\_uniform(U_s, s, n)$
    \State $X = $
    \State $U_s, s = combine(U_s, s, U_x - y d)$
    \State \Return $U_s, s, X$
\EndProcedure
\end{algorithmic}
\end{algorithm}





\section {Efficiency of entropy conversion}

In this section we'll calculate bounds on the entropy efficiency of Algortithms \ref{alg:combine}-\ref{alg:convert_b_b}, and prove that these algorithms have entropy efficiency 1 or arbitrarily close to 1.

\begin{lemma}
\label{lem:conservation}
Algorithms \ref{alg:combine}-\ref{alg:upsample} conserve entropy.
\end{lemma}

\begin{proof}
We can calculate the before and after entropy for each of these algorithms to see that they are the same. But we can also observe that since each algorithm has an inverse (Algorithm \ref{alg:combine} inverts Algorithm \ref{alg:divide}; Algorithm \ref{alg:upsample} inverts Algorithm \ref{alg:downsample}), entropy must be conserved.
\end{proof}

The \em rejection sampling \em algorithm (Algorithm \ref{alg:rejection-sampling}), and Fast Dice Roller (Algorithm \ref{alg:fast-dice-roller}) use a form of Algorithm \ref{alg:downsample}, but do lose entropy because they throw away the $B$ term. The entropy of the internal decision is exactly the entropy that is lost by these algorithms.

\begin{lemma}
    \label{lem:shannon-inequality}

For $p,q \in \mathbb{R}$, where $0 \le p\le q \le 0.5$, 

\begin{equation}
-p\log_2 p - (1-p)\log_2(1-p) \le -q\log_2 q - (1-q)\log_2(1-q)
\end{equation}
\end{lemma}

\begin{proof}
    Let
    \begin{align}
        g(p) & = -p\log_2 p - (1-p)\log_2(1-p) \\
        \implies g'(p) & = \log_2\frac{1-p}{p} = \log_2(\frac{1}{p}-1) \ge \log_21 = 0 
    \end{align}
Since the derivative of $g>0$ it means that $g$ is monotonic.
\end{proof}

\begin{definition}
    Let $H_{loss}(p)$ be the expected entropy loss function of Algorithm \ref{alg:generate-uniform}, for $p=\frac{n-1}{N}$.
\end{definition}

\begin{theorem}
    \label{thm:loss}
If $p = \frac{n-1}{N} < 0.5$,

\begin{equation}
0 \le H_{loss}(p) \le -\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{equation}

\end{theorem}

\begin{proof}
For each iteration $i$ of Algorithm \ref{alg:generate-uniform}, let $p_i = \frac{s_i \mod n}{s_i}$. But $(s_i \mod n) \le n-1$ and $s_i \ge N$, so $\frac{s_i \mod n}{s_i} \le \frac{n-1}{N}$, so $p_i \le p$. On each iteration, the entropy lost is equal to the entropy in the variable $b_i \sim Bernoulli\{p_i\}$, which is given by the entropy equation for a Bernoulli distribution \ref{todo}:

\begin{equation}
H(b_i) = -p_i\log_2p_i - (1-p_i)\log_2(1-p_i)
\end{equation}

Therefore, 

\begin{equation}
0 \le H(b_i) \le -p\log_2p - (1-p)\log_2(1-p) 
\end{equation}


by Lemma \ref{lem:shannon-inequality}. The expected number of iterations $N$ is given by

\begin{align}
& N = 1 + p_iN \le 1 + pN \\
\implies & N-pN \le 1 \\
\implies & N(1-p) \le 1 \\
\implies & N \le \frac{1}{1-p}
\end{align}

The total entropy lost by the algorithm is given by the number of iterations of the algorithm multiplied by the entropy lost in each iteration.

\begin{align}
0 \le NH(b_i) \le & \frac{1}{1-p}(-p\log_2p - (1-p)\log_2(1-p) ) \\
= & -\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{align}

We can also end up in the situation where $(s \mod n) = 0$ already, in which the \em downsample \em step always succeeds with no entropy loss, so $H_{loss}=0$.
\end{proof}

The actual entropy loss incurred by Algorithm \ref{alg:generate-uniform} depends on whatever values are found in $U_s$ and $s$, so we can only give an upper bound.

\begin{corollary}
The entropy efficiency $\eta$ of Algorithm \ref{alg:generate-uniform} is bounded by

\begin{equation}
\frac{\log_2n}{\log_2n + H_{loss}(\frac{n-1}{N})} \le \eta \le 1
\label{eq:generate-uniform-efficiency}
\end{equation}
\end{corollary}

\begin{proof}
\begin{align}
    \eta & = \frac{H_{out}}{H_{in}} \\
         & = \frac{H_{out}}{H_{out}+H_{loss}} \\
         & = \frac{\log_2n}{\log_2n + H_{loss}(\frac{n-1}{N})}
\end{align}
Therefore 
\begin{equation}
\frac{\log_2n}{\log_2n + H_{loss}(\frac{n-1}{N})} \le \eta \le 1
\end{equation}
from Theorem \ref{thm:loss}.
\end{proof}

To illustrate Equation \ref{eq:generate-uniform-efficiency}, if $N=2^{31}$ and $n=6$, then $\eta \ge 0.99999997$. This means that even with a modest entropy buffer, we can get very good entropy efficiency.

\begin{corollary}
The entropy efficiency $\eta$ of Algorithm \ref{alg:generate-uniform} is arbitrarily close to 1.
\end{corollary}

\begin{proof}
$H_{loss}(\frac{n-1}{N}) \rightarrow 0$ as $N \rightarrow \infty$. Therefore $\eta \rightarrow 1$ as $N \rightarrow \infty$.
\end{proof}


\begin{corollary}
The entropy loss of Algorithm \ref{alg:convert_u_u} is $H_{loss}(\frac{n-1}{N})$.
\end{corollary}

\begin{proof}
    Algorithm \ref{alg:convert_u_u} only loses entropy through $generate\_uniform$ which loses $H_{loss}(\frac{n-1}{N})$. The $combine$ operation does not lose entropy by Lemma \ref{lem:conservation}.
\end{proof}

\begin{corollary}
The entropy loss of Algorithm \ref{alg:generate-bernoulli} is given by $H_{loss}(\frac{n-1}{N})$.
\end{corollary}

\begin{proof}
    Algorithm \ref{alg:generate-bernoulli} only loses entropy through $generate\_uniform$ which loses $H_{loss}(n)$. The $downsample$ $combine$ operations do not lose entropy by Lemma \ref{lem:conservation}.
\end{proof}

\begin{lemma}
    \label{lem:hloss_monotonic}
    $H_{loss}(p)$ is monotonically increasing in the range $0 < p < 1$.
\end{lemma}

\begin{proof}The derivative of $H_{loss}$ is
    \begin{equation}
        \frac{-\log_2p}{(1-p)^2}
    \end{equation}
    which is positive in the range $0 < p < 1$.
\end{proof}

\begin{lemma}
The entropy loss of Algorithm \ref{alg:consume_b} is no more than than $H_{loss}(\frac{n-1}{N})$
\end{lemma}

\begin{proof}
    The entropy-losing step in Algorithm \ref{alg:consume_b} is $generate\_uniform$ which loses $H_{loss}(\frac{x-1}{N})$ bits. $x \le n$, so from Lemma \ref{lem:hloss_monotonic}, this means that $H_{loss}(\frac{x-1}{N}) \le H_{loss}(\frac{n-1}{N})$.
\end{proof}

\begin{corollary}
The amortised entropy lost by Algorithm \ref{alg:convert_b_b} is no more than $H_{loss}(\frac{n_1-1}{N}) + H_{loss}(\frac{n_2-1}{N})$
\end{corollary}

\begin{proof}
    $consume\_bit$ loses $H_{loss}(\frac{n_1-1}{N})$ bits of entropy, and $generate\_bit$ loses $H_{loss}(\frac{n_2-1}{N})$ bits of entropy.
\end{proof}









\section {Evaluation}


In terms of time, all EEC algorithms are O(1). On modern CPUs, the most expensive operation is integer division, which is usually as expensive as divide and modulus (divmod) \cite{cpudivmod}. This makes the $divide$ algorithm (Algorithm \ref{alg:divide}) the most expensive algorithm using two integer divisions, but it is still O(1).




We can compare the entropy efficiency of different algorithms. Backer et al [] compared the efficiency of random integer generation of rejection sampling (Algorithm \ref{alg:rejection-sampling}) and Knuth-Yau, implemented as Lombraso's Fast Dice Roller algorithm (Algorithm \ref{alg:fast-dice-roller}). To this comparison we can now add Algorithm \ref{alg:generate-uniform}.

Figure \ref{fig:uniform} shows the entropy efficiency when generating a single integer in the range $[0,n)$.  Algorithms that do not store entropy are much less efficient, and as observed by Backer et al \cite{todo}, the efficiency depends on the number being generated. For Algorithm \ref{alg:generate-uniform}, efficiency does decrease with the number generated according to Equation \ref{eq:generate-uniform-efficiency}, but it is not visible on this graph.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{uniform_efficiency.png}
\caption{Entropy efficiency for uniform integer generation.}
\label{fig:uniform}
\end{figure}

Figure \ref{fig:shuffle} shows the entropy efficiency when shuffling a deck of $n$ cards using the Fisher-Yates algorithm \cite{fisher-yates}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{shuffling_efficiency.png}
\caption{Entropy efficiency for card shuffling.}
\label{fig:shuffle}
\end{figure}






\section{Discussion}





\section{Conclusion}

We showed that entropy can be converted between different forms much more efficiently if we are able to pre-fetch and store entropy between invocations. Then we can exploit the large value in the entropy store to make an extremely asymmetrical descision that loses a minimum of entropy.

We have shown that the amortised entropy conversion efficiency can be arbitrarily close to 1, and the efficiency depends on the the size of the entropy store relative to the integer being generated. For applications like dice-rolling or card-shuffling, we can store the entropy in a 32-bit integer and can achieve an amortised entropy efficiency of $> 0.999999924$.

As well a generating uniform integers, we can read and generate entropy values $<1$ in the form of Bernoulli entropy, with near 1 efficiency.

This can have practical applications where the numbers generated must genuinely random and not from a pseudo-random number generator.

\printbibliography

\section {Appendix A}
Here is the source code for $generate\_uniform$, written in C.

\begin{verbatim}
    static const uint32_t N = 1<<31;
    uint32_t s_value = 0, s_range = 1;

    uint32_t generate_uniform(uint32_t n)
    {
        for(;;)
        {
            // Preload entropy one bit at a time into s
            while(s_range < N)
            {
                s_value <<= 1;
                s_value |= fetch();
                s_range <<= 1;
            }
            // Resample entropy s to a multiple of n
            uint32_t r = s_range / n;
            uint32_t c = s_range % n;
            if(s_value >= c)
            {
                // Resample successful
                s_value -= c;
                uint32_t a = s_value / n;
                uint32_t b = s_value % n;
                s_value = a;
                s_range = r; 
                return b;
            }
            else
            {
                // Resample unsuccessful
                s_range = c;
            }
        }
    }
\end{verbatim}

\end{document}
