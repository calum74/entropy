\documentclass[12pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}   % Math
\usepackage{graphicx}           % Images
% \usepackage{hyperref}           % Clickable links
\usepackage{geometry}           % Page margins
% \usepackage{cite}               % Citation formatting
\usepackage{algorithm}
\usepackage{algpseudocode}

% BibLaTeX for references (requires biber)
\usepackage[
    backend=biber,
    style=numeric,
    sorting=nyt
]{biblatex}

\addbibresource{references.bib}  % Bib file

% Page setup
\geometry{margin=1in}

% Title
\title{Algorithms for efficient entropy conversion}
\author{Calum Grant \\
OxFORD Asset Management \\
calum.grant@oxam.com}
\date{\today}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
This is a short abstract summarizing the main results and contributions of the paper. Blah blah.
\end{abstract}

\section{Introduction}

Generating uniform random integers has many applications, and there are scenarios where we want random integers that are perfectly random and perfectly distributed. True randomness comes from an external source, often delivered as a string of random bits. The external entropy must be converted to a different base that is not a power of 2, whilst consuming as few random bits as possible.

One of the first algorithms for generating uniformly distributed integers was developed by von Neumann \cite{neumann}, known as \em rejection sampling \em, and is still used to this day, for example in []. The rejection sampling algorithm, shown in Algorithm \ref{alg:rejection-sampling}, fetches random bits, either from hardware or from a pseudo-random number generator, to give a uniform integer distribution of a power of 2. Then check if the number is less than the target range n. If the number is less than n, return it, otherwise try again. This algorithm terminates with probability 1 and can be shown to generate perfectly uniform random integers.

\begin{algorithm}
\caption{Generating uniform integers using rejection sampling}
    \label{alg:rejection-sampling}
\begin{algorithmic}[1]
    \Require $n$ is an integer $>0$
    \Require $fetch()$ returns an unbounded stream of unbiassed bits
    \Ensure $v$ is uniformly distributed over $[0,n)$
\Procedure{rejection-sampling}{$n$}
    \While{true}
        \State $r \gets 1$
        \State $v \gets 0$
        \While {$r < n$}
            \State $r \gets r * 2$
            \State $v \gets v * 2 + fetch()$
        \EndWhile
        \If {$v<n$}
            \State \Return $v$
        \EndIf
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Generating uniform integers with Fast Dice Roller}
    \label{alg:fast-dice-roller}
\begin{algorithmic}[1]
    \Ensure $v$ is uniformly distributed over $[0,r)$
    \Procedure{fast-dice-roller}{$n$}
    \State $r \gets 1$
    \State $v \gets 0$
    \Loop
        \While {$r < n$}
            \State $r \gets r * 2$
            \State $v \gets v * 2 + fetch()$
        \EndWhile
        \If {$v<n$}
            \State \Return $v$
        \EndIf
        \State $r \gets r-n$
        \State $v \gets v-n$
    \EndLoop
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Related work}


\section{Algorithms for entropy conversion}

In this section we'll start with some basic operations (Algorithm \ref{alg:multiply}-\ref{alg:upsample}), then use these to create an efficient generator for uniform integers (Algorithm \ref{alg:generate-uniform}). We'll then build on Algorithm \ref{alg:generate-uniform} to create other types of entropy converters, such as converting between arbitrary uniform distributions in Algorithm \ref{alg:convert_u_u}, generate Bernoulli distributions efficiently using Algorithm \ref{alg:generate-bernoulli}, combine uniform and Bernoulli distributions using Algorithm \ref{alg:consume_b} and efficiently convert between uniform and Bernoulli distributions using Algorithm \ref{alg:convert_b_b}. What sets all of these algorithms apart is that they have a 1 or a near-1 entropy efficiency.

Algorithm \ref{alg:multiply} combines entropy in two uniform distrete random variables into a single uniform discrete random variable.

\begin{algorithm}
\caption{Multiplication of uniformly distributed integers}
\label{alg:multiply}
\begin{algorithmic}[1]
    \Require $n$, $m$, $U_n$, $U_m$ are integers
    \Require $n>0$, $m>0$
    \Require $U_n$ is uniformly distributed over $[0,n)$
    \Require $U_m$ is uniformly distributed over $[0,m)$
    \Ensure $nm$ is $n * m$
    \Ensure $U_{nm}$ is uniformly distributed over $[0,nm)$
\Procedure{multiply}{$U_n, n, U_m, m$} 
  \State $U_{nm} \gets U_n * m + U_m$
  \State $nm \gets n * m$
  \State \Return $U_{nm}, nm$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:multiply}, $U_{nm}$ is uniformly distributed over $[0,nm)$.
\label{lem:multiply}
\end{lemma}

\begin{proof}
Let $X \sim Uniform \{0 ... n-1\}$ and $Y \sim Uniform\{0 ... m-1\}$ be independent uniformly distributed random variables. The joint distribution $(X,Y)$ is uniformly distributed with $nm$ elements of probability $\frac{1}{nm}$. Let $Z$ be the distribution defined as

\begin{equation}
Z = f(X,Y) = mX+Y
\end{equation}

The mapping $f$ is a bijection between the pair $X \times Y$ and $Z$, so $Z$ is also uniformly distributed and 

\begin{equation}
Z \sim Uniform \{0 ... nm-1\}
\end{equation}
\end{proof}

Algorithm \ref{alg:divide} is the inverse of Algorithm \ref{alg:multiply}, allowing us to factorise a uniformly distributed integer into two. For this, the sizes of the output distributions must divide the size of the input distribution.

\begin{algorithm}
\caption{Division of uniformly distributed integers}
\label{alg:divide}
\begin{algorithmic}[1]
    \Require $nm$, $n$, $U_{nm}$ are integers
    \Require $nm>0$, $m>0$
    \Require $nm$ is divisible by $n$
    \Require $U_{mn}$ is uniformly distributed over $[0,nm)$
    \Ensure $n * m = nm$
    \Ensure $U_{n}$ is uniformly distributed over $[0,n)$
    \Ensure $U_{m}$ is uniformly distributed over $[0,m)$
    \Ensure $U_n$ and $U_m$ are independent
\Procedure{divide}{$U_{nm}, mn, n$} 
  \State $U_m \gets U_{nm} \operatorname{div} n$
  \State $U_{n} \gets U_{nm} \mod n$
  \State $m \gets nm / n$
  \State \Return $U_n, U_m, m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:divide}, $U_n$ is uniformly distributed over $[0,n)$ and $U_m$ is uniformly distributed over $[0,m)$. $U_m$ and $U_n$ are independent.

\label{lem:divide}
\end{lemma}

\begin{proof} $f$ is a bijection so has an inverse function 
\begin{equation}    
X = \lfloor Z/n \rfloor, Y = Z \mod n
\end{equation}

Since $X$ and $Y$ are the original random variables, they are independent and uniformly distributed.
\end{proof}

Algorithm \ref{alg:downsample} converts a uniformly distributed integer to a smaller range, and also returns a biassed bit. Unlike Algorithm \ref{alg:multiply} and Algorithm \ref{alg:divide}, the size of the output distribution depends on the value. The biassed bit also contains entropy, and in fact the total entropy returned by this algorithm is the same as its input entropy.

\begin{algorithm}
\caption{Downsampling uniformly distributed integers}
\label{alg:downsample}
\begin{algorithmic}[1]
    \Require $U_{n}$, $m$ and $n$ are integers 
    \Require $0 \le m \le n$
    \Require $U_{n}$ is uniformly distributed over $[0,n)$
\Ensure $U_{x}$ is uniformly distributed over $[0,x)$
\Ensure $x = m$ or $x=n-m$
\Ensure $B$ is a Boolean value Bernoulli distributed with $p=\frac{m}{n}$
\Ensure $U_x$ and $B$ are independent
\Procedure{downsample}{$U_n, n, m$} 
  \If{$U_n < n$}
    \State $B \gets True$  
    \State $x \gets m$
    \State $U_x \gets U_n$
  \Else
    \State $B \gets False$  
    \State $x \gets m-n$
    \State $U_x \gets U_n-m$
  \EndIf
  \State \Return $U_x, x, B$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:downsample}, $U_x$ is uniformly distributed over $[0,x)$.
\label{lem:downsample}
\end{lemma}

\begin{proof}
    Let $y = U_n$.
    $P(y<m) = \frac{m}{n}$ so $B \sim Bernoulli\{\frac{m}{n}\}$.

If $y < m$, then $y$ is uniformly distributed between $[0,m)$.

If $y \ge m$, then $y$ is uniformly distributed between $[m, n)$, so $y-m$ is uniformly distributed between $[0, n-m)$.
\end{proof}

Algorithm \ref{alg:upsample} increases the size of a uniform distribution by incorporating Bernoulli entropy. This could be used to harness entropy from biassed Bernoulli sources.

\begin{algorithm}
\caption{Upsampling uniformly distributed integers}
\label{alg:upsample}
\begin{algorithmic}[1]
\Require $U_x$ is uniformly distributed over $[0,x)$
\Require $B$ is a Boolean value Bernoulli distributed with $p=\frac{m}{n}$
\Require $x=m$ if $B$ else $x=n-m$
\Ensure $U_n$ is uniformly distributed over $[0,n)$.
\Procedure{upsample}{$U_x, x, n, B$} 
  \If{$B$}
    \State $U_n \gets U_x$  
  \Else
    \State $U_n \gets n-x+U_x$  
  \EndIf
  \State \Return $U_n$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
In Algorithm \ref{alg:upsample}, $U_{n}$ is uniformly distributed over $[0,n)$.
\end{lemma}

\begin{proof}
Let $x = U_{n} \sim X$. The possible values of $X$ are $\{0 ... n-1\}$.

Case 1: $x<n$

\begin{align}
P(x=X) & = P(B)P(x=X|B) + P(\neg B)P(x=X|\neg B) \\
       & = \frac{m}{n}\frac{1}{m} + 0 \\
       & = \frac{1}{n}
\end{align}

Case 2: $x \ge n$

\begin{align}
P(x=X) & = P(B)P(x=X|B) + P(\neg B)P(x=X|\neg B) \\
       & = 0 + (1 - \frac{m}{n})\frac{1}{n-m}  \\
       & = \frac{n - m}{n}\frac{1}{n-m} \\
       & = \frac{1}{n}
\end{align}

This means that $x \sim Uniform\{0...n-1\}$.
\end{proof}

\begin{lemma}
\label{lem:conservation}
Algorithms \ref{alg:multiply}-\ref{alg:upsample} conserve entropy.
\end{lemma}

\begin{proof}
We can calculate the before and after entropy for each of these algorithms to see that they are the same. But we can also observe that since each algorithm has an inverse (Algorithm \ref{alg:multiply} inverts Algorithm \ref{alg:divide}; Algorithm \ref{alg:upsample} inverts Algorithm \ref{alg:downsample}), entropy must be conserved.
\end{proof}

The \em rejection sampling \em algorithm (Algorithm \ref{alg:rejection-sampling}), and Fast Dice Roller (Algorithm \ref{alg:fast-dice-roller}) use a form of Algorithm \ref{alg:downsample}, but do lose entropy because they throw away the $B$ term. The entropy of the internal decision is exactly the entropy that is lost by these algorithms.

Algorithm \ref{alg:generate-uniform} reads binary entropy from a $fetch()$ function, and outputs a uniform integer in the range $[0,n)$. The algorithm makes use of an \em entropy store \em $U_s$ which is carried over in between function calls. In a practial implementation, $U_s$ and $s$ can be captured variables or class members. Initially the entropy store is empty (containing 0 entropy) with $U_s = 0$ and $s=1$.

The overall strategy of Algorithm \ref{alg:generate-uniform} is to use $downsample$ (Algorithm \ref{alg:downsample}) to ensure that $s$ is a multiple of $n$, then use the $divide$ algorithm (Algorithm \ref{alg:divide}) to divide $U_s$ into $U_n$. $U_n$ is returned as the result, and $U_s$ which is stored for the next invocation. The calculation is structured so that when $s$ is large, the entropy lost by $downsample$ is very small.

The $downsample$ on line 6 resizes $s$ to a multiple of $n$. It is overwhelmingly likely that $b$ is false because $n$ is much smaller than $s$, so we can then proceed to line 8 where we divide $U_s$ into $U_n$ and the new $U_s$. On line 9, return $U_n$ as the result and $U_s$ and $s$ as the input to the next invocation.

\begin{algorithm}
\caption{Generating uniformly distributed integers}
\label{alg:generate-uniform}
\begin{algorithmic}[1]
\Require Integers $0 < n\le N$
\Require $fetch()$ returns Bernoulli entropy with $p=0.5$
\Require $U_s$ is uniformly distributed over $[0,s)$
\Ensure $U_n$ is uniformly distributed over $[0,n)$
\Ensure $U_s$ is uniformly distributed over $[0,s)$
\Procedure{generate-uniform}{$U_s, s, n, N$} 
  \While {True}
    \While {$s < N$}
        \State $U_s, s \gets multiply(U_s, s, fetch(), 2)$
    \EndWhile
    \State $U_s, s, b \gets downsample(U_s, s, s \mod n)$ 
    \If{$ \neg b$}
        \State $U_n, U_s, s \gets divide(U_s, s, n)$
        \State \Return $U_s, s, U_n$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
    In Algorithm \ref{alg:generate-uniform}, 
$U_n$ is uniformly distributed over $[0,n)$ and 
$U_s$ is uniformly distributed over $[0,s)$.
\end{lemma}

\begin{proof}
The values $U_n$, $n$, $U_s$ and $s$ have been generated by Algorithms \ref{alg:multiply}, \ref{alg:divide} and \ref{alg:downsample}. By Lemmas \ref{lem:multiply}, \ref{lem:divide} and \ref{lem:downsample}, $U_n$ and $U_s$ are uniformly distributed.
\end{proof}

\begin{lemma}
Algorithm \ref{alg:generate-uniform} terminates with probability 1.
\end{lemma}

\begin{proof}
    If $p$ be the probability that the algorithm loops, where $p<1$, then the probability $q$ that Algorithm \ref{alg:generate-uniform} loops forever is given by $q = (1-p)q \implies q=0$.
\end{proof}

\begin{lemma}
    \label{lem:shannon-inequality}

For $p,q \in \mathbb{R}$, where $0 < p\le q < 0.5$, 

\begin{equation}
-p\log_2 p - (1-p)\log_2(1-p) \le -q\log_2 q - (1-q)\log_2(1-q)
\end{equation}
\end{lemma}

\begin{proof}
    \begin{align}
        f(p) & = -p\log_2 p - (1-p)\log_2(1-p) \\
        \implies f'(p) & = \log_2{1-p \over p} \ge \log_21 = 0 
    \end{align}
Since the derivative of $f>0$ it means that $f$ is monotonic.
\end{proof}

\begin{lemma}
    \label{lem:efficiencybinu}
If $p = \frac{n-1}{N} < 0.5$, Algorithm \ref{alg:generate-uniform} is expected to lose at most

\begin{equation}
-\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{equation}

bits of entropy.

\end{lemma}

\begin{proof}
For each iteration $i$ of Algorithm \ref{alg:generate-uniform}, let $p_i = \frac{s \mod n}{s}$. But $s \mod n \le n-1$ and $s \ge N$, so $p_i = \frac{s \mod n}{s} \le \frac{n-1}{N} = p$. On each iteration, the entropy lost equals

\begin{equation}
-p_i\log_2p_i - (1-p_i)\log_2(1-p_i) 
\le -p\log_2p - (1-p)\log_2(1-p) 
\end{equation}

by Lemma \ref{lem:shannon-inequality}. This is because the $b$ term returned by \em downsample \em is consumed by the algorithm and not returned to the caller.

The number of iterations $N$ is given by

\begin{align}
& N = 1 + p_iN < 1 + pN \\
\implies & N-pN \le 1 \\
\implies & N(1-p) \le 1 \\
\implies & N \le \frac{1}{1-p}
\end{align}

The total entropy lost by the algorithm is given by the number of iterations of the algorithm multiplied by the entropy lost in each iteration

\begin{align}
& N(-plog_2p - (1-p)\log_2(1-p)) \\
\le & \frac{1}{1-p}(-plog_2p - (1-p)\log_2(1-p) ) \\
= & -\frac{p}{1-p}\log_2p - \log_2(1-p)
\end{align}

\end{proof}

The actual entropy loss incurred by Algorithm \ref{alg:generate-uniform} depends on whatever values remained in $U_s$ and $s$, so we can only give an upper bound.

\begin{definition}
    Let the maximum entropy loss function of Algorithm \ref{alg:generate-uniform}, for $p=\frac{n-1}{N}$ be defined as
    \begin{equation}
        Maxloss(p) = -\frac{p}{1-p}\log_2p - \log_2(1-p)
    \end{equation}
\end{definition}

\begin{corollary}
The entropy efficiency of Algorithm \ref{alg:generate-uniform} is at least

\begin{equation}
\eta \ge \frac{\log_2n}{\log_n + MaxLoss(\frac{n-1}{N})}
\end{equation}
\label{cor:convert-uniform-efficiency}
\end{corollary}

To illustrate Corollary \ref{cor:convert-uniform-efficiency}, we can set $N=2^{31}$ and $n=6$. Then $\eta \ge 0.99999997$.

\begin{corollary}
The efficiency of Algorithm \ref{alg:generate-uniform} is arbitrarily close to 1.
\end{corollary}

\begin{proof}
$MaxLoss(\frac{n-1}{N}) \rightarrow 0$ as $N \rightarrow \infty$. Therefore $\eta \rightarrow 1$ as $N \rightarrow \infty$.
\end{proof}

Algorithm \ref{alg:convert_u_u} shows how we can efficiently \em convert \em entropy from one uniform distribution to another, via an entropy store.

\begin{algorithm}
\caption{Converting uniform integers}
\label{alg:convert_u_u}
\begin{algorithmic}[1]
    \Require Integers $0 < m \le N$, $U_s$, $s$
    \Require $U_s$ is uniformly distributed over $[0,s)$
    \Require $U_n$ is uniformly distributed over $[0,n)$
    \Ensure  $U_s$ is uniformly distributed over $[0,s)$
    \Ensure  $U_m$ is uniformly distributed over $[0,m)$
\Procedure{convert-uniform}{$U_s, s, U_n, n, m, N$} 
    \State $U_s, s \gets multiply(U_s, s, U_n, n)$
    \State $U_s, s, U_m \gets convertuniform(U_s, s, n, N)$
    \State \Return $U_s, s, U_m, m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Whereas Algorithm \ref{alg:generate-uniform} discards the $B$ term from $downsample$ operation, we can instead return it 

Algorithm \ref{alg:generate-bernoulli}

\begin{algorithm}
\caption{Generating biassed bits}
\label{alg:generate-bernoulli}
\begin{algorithmic}[1]
\Require Integers $0 < n \le m \le N$
\Require $U_s$ is uniformly distributed over $[0,s)$
\Ensure $U_s$ is uniformly distributed over $[0,s)$
\Ensure $B$ has a Bernoulli distribution with $p = m/n$.
\Procedure{generate-bit}{$U_s, s, m, n, N$} 
    \State $U_s, s, U_n \gets generateuniform(U_s, s, n, N)$
    \State $U_x, x, B \gets downsample(U_n, n, m)$
    \State $U_s, s \gets multiply(U_s, s, U_x, x)$
    \State \Return $U_s, s, B$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Consuming biassed bits}
\label{alg:consume_b}
\begin{algorithmic}[1]
\Procedure{consume-bit}{$U_s, s, B, m, n, N$} 
    \If {B}
        \State $x \gets m$
    \Else
        \State $x \gets n-m$
    \EndIf
    \State $U_x, U_s, s \gets generateuniform(U_s, s, x, N)$
    \State $U_s, s \gets multiply(U_s, s, U_x, x)$
    \State \Return $U_s, s$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{corollary}
The entropy efficiency of Algorithm \ref{alg:convert_u_u} is given by ....
\end{corollary}

Algorithm \ref{alg:convert_b_b} can be used to convert between Bernoulli distributions. In isolation, it makes no sense to convert a bit with bias $\frac{1}{4}$ to a bit with bias $\frac{4}{5}$, but it is perfectly possible to do this efficiently via an entropy store.

\begin{algorithm}
\caption{Converting biassed bits}
\label{alg:convert_b_b}
\begin{algorithmic}[1]
    \Require Integers ...
    \Require $U_s$ is uniformly distributed over $[0,s)$
    \Require $B_1$ is Bernoulli distributed with $p=\frac{m1}{n1}$
    \Ensure $U_s$ is uniformly distributed over $[0,s)$
    \Ensure $B_2$ is Bernoulli distributed with $p=\frac{m2}{n2}$
\Procedure{convert-bit}{$U_s, s, B_1, n1, m1, n2, m2, N$}
    \State $U_s, s = consumebit(U_s, s, n1, m1, B, N)$
    \State $U_s, s, B_2 = generatebit(U_s, s, n2, m2, N)$
    \State \Return $U_s, s, B_2$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
The entropy efficiency of Algorithm \ref{alg:convert_b_b} is given by 

\begin{equation}
$$123$$
\end{equation}

\end{lemma}

\begin{proof}
(Sum up the entropy lost by both operations)

\end{proof}

\section {Comparing integer generation methods}

We can compare the entropy efficiency of different algorithms. Backer et al [] compared the efficiency of random integer generation of rejection sampling (Algorithm \ref{alg:rejection-sampling}) and Knuth-Yau, implemented as Lombraso's Fast Dice Roller algorithm (Algorithm \ref{alg:fast-dice-roller}).

To this comparison we can now add Algorithm \ref{alg:generate-uniform}.


\section{Discussion}


\section{Mathematical Background}

Here is a sample equation:

\begin{equation}
H(p) = -p \log_2 p - (1 - p) \log_2(1 - p)
\label{eq:entropy}
\end{equation}

As shown in Equation~\ref{eq:entropy}, the binary entropy function measures the uncertainty of a Bernoulli variable.

\section{Methodology}

Describe your method here. Equations can be aligned:

\begin{align}
f(x) &= x^2 + 1 \\
f'(x) &= 2x
\end{align}

\section{Results}

Include results, tables, or figures:

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{example-image}
\caption{An example figure.}
\label{fig:example}
\end{figure}

\section{Conclusion}

Summarize your findings and potential future work.

\printbibliography

\end{document}
